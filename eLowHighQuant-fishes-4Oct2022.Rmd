---
title: "fishes:  Establishing the signal above the noise"
author: "copyright Mary Lesperance"
date: "4/10/2022"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: 3
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
geometry: margin=.5in
fontsize:  11pt
---

# R code for paper

"Establishing the signal above the noise: Accounting for an 
environmental background in the detection and quantification 
of salmonid environmental DNA", *fishes* , 2022 by 
Morgan D. Hocking, Jeffrey C. MacAdams, Michael J. Allison, Lauren C. Bergman, Robert Sneiderman
Ben F. Koop, Brian M. Starzomski, Mary L. Lesperance and Caren C. Helbing; 7: 266. DOI: 10.3390/fishes7050266

# Outline 
**Develop Low/High Copy Standard curves for eONKI4; apply to Density and Dilution series**

1.  (a) Use eLowQuant code (copied here) with gBlock data for eONKI4 
to develop a standard curve for the samples with less than 100% detections. 
We used the no-intercept model, however, code for the intercept model is included here. 
The eONKI4 gBlock data is in file GEDWG_LOD_DATA3.csv.
CalibS0.table and CalibS0.table0 contain the SQ estimtes SQ0's and SE_SQ0's as well as
model estimates and their standard errors for the intercept and no intercept models
respectively for sample sizes 8, 16, 24, 32.  The Limits of Blank (LOB) and 
Detection (LOD) are computed
using the Binomial-Poisson model.

1.  (b) Weighted linear regression is used to develop a standard curve for the high copy
data using gBlock Cq data, GEDWG_LOD_DATA3.csv, eONKI4 data.

2.  The Density Experiment - The weighted linear regression model is used to estimate
copy number.  Log_2 copy number is plotted versus log_2 biomass, and the relationship
is modelled using another weighted linear regression model.

3. The Dilution Experiment - Both standard curves are used to estimate copy numbers
for the Dilution experiment.  

4.  The Background eDNA copy number is estimated using the negative controls
(SQ=0) from both the Density and Dilution experiments.

5. The Limits of Blank and Detection, adjusted for Background, (LOB-B and LOD-B)
are computed using the Binomial-Poisson model and the estimated Background eDNA.

6.  The Dilution Experiment log_2 copy number estimates are plotted versus 
log_2 flow together with the adjusted limits of blank and detection.  A Bent Cable
model is fitted to the data.




# First run eLowQuant with GEDWG_LOD_DATA#.csv - Extract eONKI4

## eLowQuant Instructions copied here
This file performs the computations in the publication, 
'A Statistical Model for Calibration and Computation of Detection
and Quantification Limits for Low Copy Number Environmental
DNA samples' by Lesperance, Allison, Bergman, Hocking, Helbing, 
*Environmental DNA*, 2021, 00, 1-12 https://doi.org/10.1002/edn3.220.

* Create a folder call Outputs in your working directory.
* Put your data file in your working directory and put the name
of the file in the chunk labeled 'READIN' below.
* Data set csv file requirements.  Columns:  Target, Lab, Cq, SQ
* For nondetects, set Cq to be empty or NA value
* For negative controls, set Sq to be empty or 0 or NA value
* Include negative controls (no template controls or NTCs) in the csv file!
* DO NOT DUPLICATE Target names over different Labs!!

* This code uses observations with nonempty data and where phat <1 (num detect<num technical replicates)
* Only the SQ's up to the first one with  phat==1 are used.
* Allows for variable numbers of SQ levels per Target.
* Assumes SQs == NA are zero, i.e. are negative controls.


* The code uses the R function optim.  A convergence code 0 indicates successful completion.  
* Ignore warnings if results are sensible.

* EXECUTE EACH CHUNK LOOKING AT THE OUTPUT.  IN PARTICULAR, LOOK AT THE GRAPHS IN
THE CHUNK CALLED 'PlotPois' TO DETERMINE IF THE MODEL IS APPROPRIATE.  IT IS NOT
APPROPRIATE IF 'lm Rsq' is small, i.e. near zero!

* You can send results to files by setting the sink.indicator and Manusink to TRUE and running the code in RStudio.  This currently does not work for all results files when knitting.


* If you wish to knit to pdf AND you do NOT have a version of Latex installed
on your computer, then run the following in your RStudio console:  
install.packages(“tinytex”); tinytex::install_tinytex()

```{r Packages, include=FALSE}
rm(list=ls())

# Packages used; SiZer<-bent.cable model
packages = c("dplyr","ggplot2","knitr","kableExtra","RColorBrewer", 
             "SiZer", "boot", "latex2exp")

## Load or install&load
package.check <- lapply(packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

## Source the functions
source("eLowQuant-Functions-V20210407.R")

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=7, fig.width=6.5)
knitr::opts_chunk$set(fig.pos = '!h')
#sink.indicator <- FALSE   #sinks results to files if TRUE
sink.indicator <- FALSE
```

## Read in Data

Data should be in the form of a csv file with column names:  Target, Lab, Cq, SQ.

Target is the name of the assay,   
Lab is the lab name,   
Cq is the cycle threshold where a *missing* Cq is a reaction *nondetect*,      
SQ is the starting quantity where missing values are assumed to be zero.

It is important to include the negative controls (i.e. SQ=0)!


```{r READIN, include=FALSE}
## READIN paragraph adapted from Merkes, Christopher <cmerkes@usgs.gov>
##   Used only to read in the data.


## Read in your data file (MODIFY FILE NAME AS NEEDED):
DAT <- read.csv("GEDWG_LOD_DATA3.csv")
dim(DAT)

## Create an analysis log file:
write(paste0("Analysis started: ",date(),"\n\n"),file="Outputs\\Analysis Log.txt")

## Check the data:
if(sum(colnames(DAT)=="Target")!=1) { #Is there a "Target" column?
  A <- grep("target", colnames(DAT), ignore.case=T)
  if(length(A)==1) { colnames(DAT)[A] <- "Target" } #Rename target column if it is mispelled but can be identified and there is only 1.
  if(length(A)!=1) { write("There is a problem with the 'Target' column.\n\n",file="Analysis Log.txt",append=T) } #Add error message to analysis log.
  if(length(A)>1) { cat("ERROR: multiple 'Target' columns detected.",colnames(DAT)[A],sep="\n") }
  if(length(A)==0) { print("ERROR: cannot detect 'Target' column.") }
}

if(sum(colnames(DAT)=="Lab")!=1) { #Is there a "Lab" column?
  A <- grep("lab", colnames(DAT), ignore.case=T)
  if(length(A)==1) { colnames(DAT)[A] <- "Lab" } #Rename Lab column if it is mispelled but can be identified and there is only 1.
  if(length(A)!=1) { write("There is a problem with the 'Lab' column.\n\n",file="Analysis Log.txt",append=T) } #Add error message to analysis log.
  if(length(A)>1) { cat("ERROR: multiple 'Lab' columns detected.",colnames(DAT)[A],sep="\n") }
  if(length(A)==0) { print("ERROR: cannot detect 'Lab' column.") }
}

if(sum(colnames(DAT)=="Cq")!=1) { #Is there a "Cq" column?
  A <- grep("cq|ct|cycle",colnames(DAT),ignore.case=T)
  if(length(A)==1) { colnames(DAT)[A] <- "Cq" } #Rename cq column if it is mispelled but can be identified and there is only 1.
  if(length(A)!=1) { write("There is a problem with the 'Cq' column.\n\n",file="Analysis Log.txt",append=T) } #Add error message to analysis log.
  if(length(A)>1) { cat("ERROR: multiple 'Cq' columns detected.",colnames(DAT)[A],sep="\n") }
  if(length(A)==0) { print("ERROR: cannot detect 'Cq' column.") }
}

if(sum(colnames(DAT)=="SQ")!=1) { #Is there a "SQ" column?
  A <- grep("sq|copies|starting|quantity",colnames(DAT),ignore.case=T)
  if(length(A)==1) { colnames(DAT)[A] <- "SQ" } #Rename SQ column if it is mispelled but can be identified and there is only 1.
  if(length(A)!=1) { write("There is a problem with the 'SQ' column.\n\n",file="Analysis Log.txt",append=T) } #Add error message to analysis log.
  if(length(A)>1) { cat("ERROR: multiple 'SQ' columns detected.",colnames(DAT)[A],sep="\n") }
  if(length(A)==0) { print("ERROR: cannot detect 'SQ' column.") }
}

## Ensure data is in the proper format:
DAT$Target <- as.factor(DAT$Target)
DAT$Lab <- as.factor(DAT$Lab)  #ML
DAT$Cq <- suppressWarnings(as.numeric(as.character(DAT$Cq))) #Non-numerical values (i.e. negative wells) will be converted to NAs
DAT$SQ <- suppressWarnings(as.numeric(as.character(DAT$SQ))) #Non-numerical values (i.e. NTC) will be converted to NAs

## ML Remove positive controls
# dim(DAT)
# DAT <- DAT[DAT$Content!='Pos Ctrl',]
# dim(DAT)

## ML assume SQs == NA are zero - negative controls
DAT$SQ[is.na(DAT$SQ)] <- 0
dim(DAT)
summary(DAT)
DAT.df <- data.frame(DAT)

## ML Chris omitted negative controls from data set
## ML negative controls will be added to CERC, Monroe, UVIC below

```

## Pick out the eONKI4  data only 

```{r eONKI4neFish, eval=TRUE, echo=FALSE}
# dim(DAT.df)
DAT.df <- droplevels(DAT.df[is.element(DAT.df$Target,c("eONKI4")), ])

cat('Number row and columns:  ',dim(DAT.df),"\n", "Data Summary", "\n")
summary(DAT.df)
```


## Process/Summarize samples by Target/Sq; Compute the Poisson estimates of Sq

Hindson et al "High-Throughput Droplet Digital PCR System for Absolute 
Quantitation of DNA Copy Number", Anal. Chem., 2011, 83 (22), pp 8604–8610
use a Poisson approximation for quantification.
Before that, Dube et al. 2008, "Mathematical analysis of copy number 
variation in a DNA sample
using digital PCR on a nanofluidic device", PloS One, Vol 3, Issue 8, e2876,
model the number of molecules in each chamber as a Poisson process, giving the 
relationship between $p$ and $\lambda$.

```{r detect, warning=FALSE, message=FALSE, echo=FALSE}
## Summarize data by (Target, SQ), detect=#detections, n=#tech reps
DAT.Tar.SQ <-  DAT.df %>%
  group_by(Target, SQ) %>%
  summarise(detect=sum(!is.na(Cq)), n=n(),  Cqmean=mean(Cq, na.rm=TRUE), 
            Lab=Lab[1] )  #.groups="keep"
DAT.Tar.SQ <- droplevels(data.frame(DAT.Tar.SQ))
#dim(DAT.Tar.SQ); cat('dim, before negative controls added')
#summary(DAT.Tar.SQ)

uLabs <- unique(DAT.Tar.SQ$Lab) #unique labs

## ML check the data for each Lab
# if(sink.indicator){
#   sink(file='Outputs\\SummariesRaw.txt', split=TRUE)}
# for(i in uLabs){
#   print(knitr::kable(DAT.Tar.SQ[DAT.Tar.SQ$Lab==i,], format="pandoc", 
#   digits=3, caption=i), results="asis")
# }
# if(sink.indicator){ sink()}

## All labs had negative controls, some of which were omitted in gBlock file
## If lab has no technical replicates with SQ=0, 
## Add in Negative Control (ntc) zeroes (24/(48 for Monroe) technical replicates)

ntc.rows <- tibble(Target=factor(0), SQ=0, detect=0, n=0, Cqmean=0, Lab=factor(0))
for (i in unique(DAT.Tar.SQ$Target)){
  nn=24
  if ((DAT.Tar.SQ$Lab[DAT.Tar.SQ$Target==i])[1]=="Monroe") nn=48
  if(min(DAT.Tar.SQ$SQ[DAT.Tar.SQ$Target==i])!=0){
       ntc.rows <- ntc.rows %>% add_row(Target=i,
                                     SQ=0, detect=0, n=nn, Cqmean=NA,
                                     Lab=(DAT.Tar.SQ$Lab[DAT.Tar.SQ$Target==i])[1])
  }
}
ntc.rows <- data.frame(ntc.rows[-1,])  #remove the first row
# DAT.Tar.SQ <- DAT.Tar.SQ %>% add_row(ntc.rows)  #no longer works
suppressWarnings(DAT.Tar.SQ <- DAT.Tar.SQ %>% bind_rows(ntc.rows))
DAT.Tar.SQ$Target <- as.factor(DAT.Tar.SQ$Target)
DAT.Tar.SQ$Lab <- as.factor(DAT.Tar.SQ$Lab)
# cat('dim, Added 24/48 negative controls for Targets with zero ntc') 

DAT.Tar.SQ <- arrange(DAT.Tar.SQ, Lab, Target, SQ) #sort data by SQ in Target in Lab

#write.csv(DAT.Tar.SQ, "DAT.Tar.SQ.csv")  #write the data to file

## Add variables to data set:  L10.SQ, phat, ... 
DAT.Tar.SQ <- within(DAT.Tar.SQ, {
  L10.SQ <- log10(SQ)  
  phat <- detect/n           #sample proportion detect
  vphat <- phat*(1-phat)/n   #var of phat
  lamhat <- -log(1-phat) 
  vlamhat <- phat/n/(1-phat)  #var of lamhat using the delta method
  sdlamhat <- sqrt(vlamhat)   #sd of lamhat using the delta method
  MElamhat <- 1.96*sdlamhat  #margin of error for lambda hat using delta method
}
)
dim(DAT.Tar.SQ) 


## All Targets and Labs **DO NOT DUPLICATE Target names over Labs!!
uLabs <- unique(DAT.Tar.SQ$Lab)
uTargets <- unique(DAT.Tar.SQ$Target)
nTargets <- length(uTargets)
uLabsTargets <- unique(DAT.Tar.SQ[,c('Lab','Target')])
uLabsTargets$Lab <- as.character(uLabsTargets$Lab)
#ensure ulabsTargets in same order as uTargets
uLabsTargets <- uLabsTargets[match(uLabsTargets$Target, uTargets),]  
uLabsTargets.names <- apply(uLabsTargets, 1, paste, collapse=', ')

## Print summary tables.  If sink.indicator set to TRUE, writes tables to file.
{
if(sink.indicator){
  sink(file='Outputs\\Summaries.txt', split=TRUE)}
for(i in uLabs){
  print(knitr::kable(DAT.Tar.SQ[DAT.Tar.SQ$Lab==i, c(1:4,10, 12)], format="pandoc", digits=3, caption=i),
        results="asis")
}
if(sink.indicator)  sink()
}

```



```{r ExactTransCI, echo=FALSE}
## Exact 95% Binomial Confidence intervals => backtransform given alpha and beta
## Binomial bound from Julious 2005, Stat in Medicine

DAT.Tar.SQ <- within(DAT.Tar.SQ, {
  CIexphat.lower <-  1 - qbeta(.975, n-detect+1, detect)  #exact phat bounds
  CIexphat.upper <-  qbeta(.975, detect+1, n-detect)

## Use transformed exact phat bounds
  Lamhatex.Lower <- -log(1 - CIexphat.lower)
  Lamhatex.Upper <- -log(1 - CIexphat.upper)
}
)

```

## Plot the Poisson estimates (and CI) of Sq for levels that had non-detects
Only the first levels of SQ that had non-detects are analyzed. 
Red line is least squares linear regression line.

LOOK AT THE GRAPHS IN
THE CHUNK CALLED 'PlotPois' TO DETERMINE IF THE MODEL IS APPROPRIATE.  IT IS NOT
APPROPRIATE IF 'lm Rsq', the linear model R-square, is small, i.e. near zero!


```{r PlotPois, warning=FALSE, echo=FALSE}
#ML  ??will error if all phats==1

#Graphs can be saved as pdf or postscript using the codes below.  
# Remember to run dev.off() at the bottom if you send graphs to file.
#pdf('Outputs\\lamhat.pdf')
#postscript('\\Outputs\\lamhat.eps')

#Plots 2 by 2 on a page if there are more than 3 assays; plots 2
# on a page if there are 2 assays.
#Comment out the two lines below to put one plot on a page.
 if(nTargets>3) par(mfrow=c(2,2))
 if(nTargets==2) par(mfrow=c(1,2))

## Use observations with nonempty data and where phat <1
## Only the SQ's before the first one with phat==1 are used
## Allows for variable numbers of SQ levels per Target

nndetect <- vector("list", nTargets) 
nrowTarget <- rep(0, length=nTargets)

for(i in 1:nTargets) {
  Target.dat <- subset(DAT.Tar.SQ, Target==uTargets[i])
  bSQ <- !is.na(Target.dat$phat)  
  lastSQ <- as.logical(cumprod(Target.dat$phat!=1 & bSQ)) 
## removes first observations with SQ with phat=1 and larger SQs
  Target.dat <- Target.dat[lastSQ,]
  nndetect[i] <- list(Target.dat )
  nrowTarget[i] <- nrow(Target.dat)
 
  if(nrow(nndetect[[i]]) < 2) {cat(paste('Too few values for ', uTargets[i])); next}
  
  maxSQ <- max(Target.dat$SQ)
  maxlamhat <- max(Target.dat$lamhat)
  plot(Target.dat$SQ, Target.dat$lamhat, xlog=TRUE, ylab='Lambda hat',
       xlab='Starting copy number',
       ylim=c(0, maxlamhat), xlim=c(0, maxSQ), main=uLabsTargets.names[i])
## Transformed Exact CI
  arrows(Target.dat$SQ, Target.dat$Lamhatex.Lower, Target.dat$SQ, 
         Target.dat$Lamhatex.Upper,
         length=0.05, angle=90, code=3)
## overlay simple regression line and R-squared
  jlm <- lm(lamhat ~ SQ, data=Target.dat)
  abline(jlm, col=2)
  legend("topleft", paste('lm Rsq=',round(summary(jlm)$r.squared, 2)), bty="n")
  cat("\n\n")
}


#dev.off()
par(mfrow=c(1,1))
```


\pagebreak

Both the intercept and no intercept models are fit to the data.  The 'best' of
the two models is the one with the largest Likelihood Ratio test p-value.
The 'best' model will be identified in the chunk called *Manuscript*.

## Estimate Poisson models - no intercept model

```{r MLEfit0s, warning=FALSE, echo=FALSE, fig.height=4.5, fig.width=6.5}
#Graphs can be saved as pdf or postscript using the codes below.  
# Remember to run dev.off() at the bottom if you send graphs to file.
#pdf('Outputs\\MLfit0.pdf')
#postscript('Outputs\\MLfit.eps')

#Puts two plots side-by-side on a page; Comment out for one on a page
par(mfrow=c(1,2))

## List of results - 0 in name denotes fits with no intercept
Calib.fit.estimates0 <- vector("list", nTargets)  #list of fit estimates
names(Calib.fit.estimates0) <- uTargets
Calib.fit.all0 <- vector("list", nTargets)  #list of fits
Calib.fit.LLRp0 <- vector("numeric", nTargets)
names(Calib.fit.LLRp0) <- uTargets
Calib.fit.res0 <- matrix(0, nrow=nTargets, ncol=4)
rownames(Calib.fit.res0) <- uTargets
colnames(Calib.fit.res0) <- c("convergence", "LLR", "degf","Pval")
Calib.fit.res0 <- data.frame(Calib.fit.res0)


for(i in 1:nTargets){
  if(nrow(nndetect[[i]]) < 3) {Calib.fit.estimates0[[i]] <- NULL
    cat(paste('Too few values for ', uTargets[i]), '\n')
    next}
  Target.dat <- nndetect[[i]]
  
  # Could use following for starting value
  # j.glm <- glm(cbind(n-detect, detect)~SQ-1, data=Target.dat, family=binomial(link='log'))


   Calib.fit <- optim(par=c(1), fn=CalibOr.LLik,  nd=Target.dat$detect,
                     S=Target.dat$SQ, nn=Target.dat$n,
                     method="BFGS", control=list(fnscale=-1), gr=CalibOr.dLLik,
                     hessian=TRUE)
  
   Calib.fit.all0[[i]] <- Calib.fit
   
## Variance estimates
   if(nrow(Target.dat)==2) {
    Calib.fit.Var <- matrix(0, 2, 2)  #singular hessian for 2 observations
    }  else { 
    Calib.fit.Var <- solve(-Calib.fit$hessian)
    }
  cmat <- cbind(Calib.fit$par, sqrt(diag(Calib.fit.Var)))
  cmat <- cbind(cmat, cmat[,1]/cmat[,2])
  cmat <- cbind(cmat, 2*pnorm(-cmat[, 3]))
  colnames(cmat) <- c("Estimate", "Std.Err", "Z value", "Pr(>z)")
  if (nrow(cmat)==1) {rownames(cmat) <- c("beta")
  } else  rownames(cmat) <- c("alpha","beta")
  Calib.fit.estimates0[[i]] <- cmat
  
## Likelihood ratio statistic and pvalue for goodnes-of-fit of the model
  Calib.degf <- nrow(Target.dat) - length(Calib.fit$par)
  #SQ=0 do not contribute to the likelihood for no intercept model
  bool <- Target.dat$SQ !=0 
  Calib.LLR <- 2*(Bin.LLik(Target.dat$detect[bool], Target.dat$n[bool]) 
                  - Calib.fit$value)  
  Calib.LLR.pv <- pchisq(Calib.LLR, Calib.degf, lower.tail = FALSE)
  Calib.fit.LLRp0[i] <- Calib.LLR.pv
  Calib.fit.res0[i,] <- c(Calib.fit$convergence, Calib.LLR, Calib.degf, Calib.LLR.pv)
   

## Compute fitted values for ML model
  Calib.fitted <- Calib.fit$par *  Target.dat$SQ
  
  # Plot calibration curve on lambda scale
  maxSQ <- max(Target.dat$SQ)
  maxlamhat <- max(Target.dat$lamhat)

  plot(Target.dat$SQ, Target.dat$lamhat, xlog=TRUE, 
       ylab='Mean copy estimate', xlab='Starting copy number',
       ylim=c(0, maxlamhat), xlim=c(0, maxSQ), las=1,
  #     main='A')
      main=uTargets[i])
  abline(0, Calib.fit$par, col=4)
  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  arrows(Target.dat$SQ, Target.dat$Lamhatex.Lower, Target.dat$SQ, 
         Target.dat$Lamhatex.Upper,
         length=0.05, angle=90, code=3)

# png(filename="Poisson-Binomial.png", res=200) # perhaps width/height as well
# par(mfrow=c(2,1))
# # ...
# dev.off()

## Plot calibration curve on phat scale
## Compute minSQ such that phat~=1 (1-phat = .99)
##  sqs <- seq(0, maxSQ)
  maxSQa <- max( -( log(.01))/Calib.fit$par, maxSQ)
  sqs <- seq(0, maxSQa, by=.1)
  Calib.phat <- 1 - exp(-( Calib.fit$par * sqs))

  plot(sqs, Calib.phat, xlog=TRUE, 
       ylab='Proportion detect', xlab='Starting copy number per reaction', 
       type='l', col=1, las=1, 
       ylim=c(0, 1), xlim=c(0, maxSQa),  
#      main='B')
      main=uTargets[i])
  points(Target.dat$SQ, Target.dat$phat)
#  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
#  legend("topleft", legend=c('B'), bty="n")

# #Hatchery paper, Plot the transpose of the calibration plot on phat scale
#   plot( Calib.phat, sqs, xlog=TRUE, 
#        xlab='Proportion detect', ylab='Starting copy number per reaction', 
#        type='l', col=1, las=1, 
#        xlim=c(0, 1), ylim=c(0, maxSQa), 
# #      main='B')
#       main=uTargets[i])
#   points(Target.dat$phat, Target.dat$SQ)
# #  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
# #  legend("topleft", legend=c('D'), bty="n")


cat('\n\n\n')

## If desired, set sink.indicator to TRUE to write results to file.  
  if(sink.indicator){
    sink(file(paste("Outputs\\MLNoInter",uTargets[i],".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  }
  cat("\n", as.character(uTargets[i]), "\n")
  cat("Convergence=", Calib.fit$convergence, "\n")
  printCoefmat(cmat, digits=3)
  cat('LLR test stat=', Calib.LLR, ', df= ', Calib.degf, ', p-value=', Calib.LLR.pv, "\n\n")
  if(sink.indicator) sink()

knitr::asis_output("\n\\newpage\n")


}
  
#dev.off()
par(mfrow=c(1,1))
  
``` 
  
\newpage
  
## Estimate predicted SQ given number detects and technical replicates - no intercept (not shown)
The estimated SQ is easily obtained for given new values of nn0=number of replicates, 
nd0=number detected and the estimated slope betaS 
as:    Shat <- -(log((nn0 - nd0)/nn0)) / betaS[1]
The standard errors are obtained from the Hessian matrix 
(or via the function CalibS0Or.ddLLik()) .


```{r MLES0fits0, warning=FALSE, echo=FALSE, eval=FALSE, include=FALSE}  

  #Calibration estimate of S0 given new nd0=number detected, nn0=number replicates
  # includes standard errors and p-values

  CalibS0.fit.estimates0 <- vector("list", nTargets)  #list of fits including test value
  
  for(i in 1:nTargets){
#  nd0 <- 32; nn0 <- 96  #nd0test number detects, nn0=number technical replicates
   nd0 <- 3; nn0 <- 8
#  nd0 <- 1; nn0 <- 3   

     if(nrow(nndetect[[i]]) < 3) {CalibS0.fit.estimates0[[i]] <- NULL
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}
   Target.dat <- nndetect[[i]]


  CalibS0.fit <- optim(par=c(1, 3), fn=CalibS0Or.LLik,  nd=Target.dat$detect,
                     S=Target.dat$SQ, nn=Target.dat$n, nd0=nd0, nn0=nn0,
                     method="BFGS", control=list(fnscale=-1), gr=CalibS0Or.dLLik, 
                     hessian=TRUE)

  CalibS0.fit.Var <- solve(-CalibS0.fit$hessian)
  betaS <- CalibS0.fit$par
  cmat <- cbind(betaS, sqrt(diag(CalibS0.fit.Var)))
  cmat <- cbind(cmat, cmat[,1]/cmat[,2])
  cmat <- cbind(cmat, 2*pnorm(-cmat[, 3]))
  if (nrow(cmat)==2) {rownames(cmat) <- c("beta", "SQ0")
  } else  rownames(cmat) <- c("alpha","beta", "SQ0")
  colnames(cmat) <- c("Estimate", "Std.Err", "Z value", "Pr(>z)")

  cat('\n\n','ML estimate of SQ for nd0 number of detects and nn0 replicates', '\n')
  if(sink.indicator) sink(file(paste("Outputs\\MLNoInterSQ",uTargets[i],
            ".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  cat(as.character(uTargets[i]), 'y0=', nd0, 'n0=' , nn0, "\n\n")
  printCoefmat(cmat, digits=3)
  if(sink.indicator) sink()
  
  CalibS0.fit.estimates0[[i]] <- cmat
  
  }

```

## Estimate predicted SQ given consecutive numbers of detects given number of technical replicates - no intercept
The estimated SQ is easily obtained for given new values of nn0=number of replicates, 
nd0=number detected and the estimated slope betaS 
as:    Shat <- -(log((nn0 - nd0)/nn0)) / betaS[1]
The standard errors are obtained from the Hessian matrix 
(or via the function CalibS0Or.ddLLik()).



```{r MLES0fits0vec, warning=FALSE, echo=FALSE}  

  #Calibration estimate of S0 given given consecutive numbers of detects given vector
  # number of technical replicates nn0vec0 - no intercept model
  # includes standard errors

i <- 1 #ML Only 1 Target used here, eONKI4, the i'th Target

nn0vec0 <- c(8, 16, 24, 32)  #ML cycle through vector number of technical replicates
CalibS0.table0 <- vector("list", length(nn0vec0))  #list of fits including test value
  for(j in 1:length(nn0vec0)) {
     nn0 <- nn0vec0[j] #number of technical replicates
     if(nrow(nndetect[[i]]) < 3) {CalibS0.table0[[i]] <- NULL
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}
   
  Target.dat <- nndetect[[i]]
  SQS0.fit0 <- matrix(0, ncol=4, nrow=0)
  
  for(nd0 in 0:(nn0-1)){

      CalibS0.fit <- optim(par=c(1, 3), fn=CalibS0Or.LLik,  nd=Target.dat$detect,
                     S=Target.dat$SQ, nn=Target.dat$n, nd0=nd0, nn0=nn0,
                     method="BFGS", control=list(fnscale=-1), gr=CalibS0Or.dLLik, 
                     hessian=TRUE)

  CalibS0.fit.Var <- solve(-CalibS0.fit$hessian)
  betaS <- CalibS0.fit$par
  cvec <- as.vector(rbind(betaS, sqrt(diag(CalibS0.fit.Var))))
  SQS0.fit0 <- rbind(SQS0.fit0, cvec)
  }
  
  rownames(SQS0.fit0) <- paste(0:(nn0-1))
  colnames(SQS0.fit0) <- c("alpha", "SEalpha", "SQ0","SE_SQ0")

  # Negative SQ0 values can occur when there are detects for negative controls(?)
  # set SQ0 and SE_SQ0 to zero
  SQS0.fit0[ SQS0.fit0[,3] <= 0, 3:4] <- 0
  
  cat("\n",as.character(uTargets[i]), "\n")
  cat(' ML estimate of SQ for numbers of detects and', nn0, 'replicates', '\n')
  if(sink.indicator) sink(file(paste("Outputs\\MLNoInterSQ0",uTargets[i],
            ".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  print(knitr::kable(cbind(NumDetects=0:(nn0-1), SQS0.fit0[,3:4]), 
                     format="pandoc", digits=4),results="asis")

  if(sink.indicator) sink()
  
  CalibS0.table0[[j]] <- SQS0.fit0
 
  }

```


\newpage

## Estimate Poisson models - intercept model 

```{r MLEfits, warning=FALSE, echo=FALSE, fig.height=4.5, fig.width=6.5}
#pdf('Outputs\\MLfit.pdf')
#postscript('Outputs\\MLfit.eps')

# if(nTargets>3) par(mfrow=c(2,2))
# if(nTargets==2) par(mfrow=c(1,2))
par(mfrow=c(1,2))

# List of results
Calib.fit.estimates <- vector("list", nTargets)  #list of fit estimates
names(Calib.fit.estimates) <- uTargets
Calib.fit.all <- vector("list", nTargets)  #list of fits all
Calib.fit.LLRp <- vector("numeric", nTargets)
names(Calib.fit.LLRp) <- uTargets
Calib.fit.res <- matrix(0, nrow=nTargets, ncol=4)
rownames(Calib.fit.res) <- uTargets
colnames(Calib.fit.res) <- c("convergence", "LLR", "degf","Pval")
Calib.fit.res <- data.frame(Calib.fit.res)

for(i in 1:nTargets){
  if(nrow(nndetect[[i]]) < 3) {Calib.fit.estimates[[i]] <- NULL
     cat(paste('Too few values for ', uTargets[i]), '\n')
    next}
  Target.dat <- nndetect[[i]]
  jlm <- lm(lamhat ~ SQ, data=Target.dat) #starting values for alpha and beta

  Calib.fit <- optim(par=pmax(c(0.01, 0.01), coef(jlm)), fn=Calib.LLik,  
                     nd=Target.dat$detect,
                     S=Target.dat$SQ, nn=Target.dat$n, gr=Calib.dLLik,
                     method="BFGS", control=list(fnscale=-1),  hessian=TRUE)

  Calib.fit.all[[i]] <- Calib.fit
  if(nrow(Target.dat)==2) {
    Calib.fit.Var <- matrix(0, 2, 2)  #singular hessian for 2 observations
    }  else { 
    Calib.fit.Var <- solve(-Calib.fit$hessian)
    }
  cmat <- cbind(Calib.fit$par, sqrt(diag(Calib.fit.Var)))
  cmat <- cbind(cmat, cmat[,1]/cmat[,2])
  cmat <- cbind(cmat, 2*pnorm(-cmat[, 3]))
  colnames(cmat) <- c("Estimate", "Std.Err", "Z value", "Pr(>z)")
  if (nrow(cmat)==1) {rownames(cmat) <- c("beta")
  } else  rownames(cmat) <- c("alpha","beta")
  Calib.fit.estimates[[i]] <- cmat
  
  #Likelihood ratio statistic and pvalue for goodnes-of-fit of the model
  Calib.degf <- nrow(Target.dat) - length(Calib.fit$par)
  Calib.LLR <- 2*(Bin.LLik(Target.dat$detect, Target.dat$n) 
                  - Calib.fit$value)  
  Calib.LLR.pv <- pchisq(Calib.LLR, Calib.degf, lower.tail = FALSE)
  Calib.fit.LLRp[i] <- Calib.LLR.pv
  Calib.fit.res[i,] <- c(Calib.fit$convergence, Calib.LLR, Calib.degf, Calib.LLR.pv)
  
  #Compute fitted values for ML model
  Calib.fitted <- Calib.fit$par[1] + Calib.fit$par[2]*  Target.dat$SQ  

  # Plot calibration curve on lambda scale
  maxSQ <- max(Target.dat$SQ)
  maxlamhat <- max(Target.dat$lamhat)

  plot(Target.dat$SQ, Target.dat$lamhat, xlog=TRUE, 
       ylab='Mean copy estimate', xlab='Starting copy number',
       ylim=c(0, maxlamhat), xlim=c(0, maxSQ),  las=1,
 #      main='A'
       main=uTargets[i]
       )
  abline(Calib.fit$par[1], Calib.fit$par[2], col=4)
  #ML legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  arrows(Target.dat$SQ, Target.dat$Lamhatex.Lower, Target.dat$SQ, 
         Target.dat$Lamhatex.Upper,
         length=0.05, angle=90, code=3)
 
  # Plot calibration curve on phat scale
  # Compute minSQ such that phat~=1
  maxSQa <-max( -(Calib.fit$par[1] + log(.01))/Calib.fit$par[2], maxSQ)
  sqs <- seq(0, maxSQa, by=.1)
  Calib.phat <- 1 - exp(-(Calib.fit$par[1] + Calib.fit$par[2] * sqs))

  plot(sqs, Calib.phat, xlog=TRUE, 
       ylab='Proportion detect', xlab='Starting copy number', type='l', col=4, las=1, 
       ylim=c(0, 1), xlim=c(0, maxSQa),
#       main='B'
       main=uTargets[i]
       )
  points(Target.dat$SQ, Target.dat$phat)
  #ML legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")

cat("\n\n\n")

## If desired, write results to file.
  if(sink.indicator){
    sink(file(paste("Outputs\\ML",uTargets[i],".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  }
  cat("\n", as.character(uTargets[i]), "\n")
  cat("Convergence=", Calib.fit$convergence, "\n")
  printCoefmat(cmat, digits=3)
  cat('LLR test stat=', Calib.LLR, ', df= ', Calib.degf, ', p-value=', Calib.LLR.pv, "\n\n")
  if(sink.indicator) sink()

  knitr::asis_output("\n\\newpage\n")

}   
#dev.off()
par(mfrow=c(1,1))
  
``` 

\newpage

## Estimate predicted Sq given number detects and technical replicates - intercept model (not shown) 
  

```{r MLES0fits, warning=FALSE, echo=FALSE, eval=FALSE, include=FALSE}  

  #Calibration estimate of S0 given new nd0=number detected, nn0=number replicates
  # includes standard errors and p-values

  CalibS0.fit.estimates <- vector("list", nTargets)  #list of fits including test value
  
  for(i in 1:nTargets){
   nd0 <- 22; nn0 <- 42  #test number detects, number technical replicates
   if(nrow(nndetect[[i]]) < 3) {CalibS0.fit.estimates[[i]] <- NULL
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}
   Target.dat <- nndetect[[i]]
   jlm <- lm(lamhat ~ SQ, data=Target.dat) #starting values for alpha and beta
   jpar <- pmax(c(0.01, 0.01), coef(jlm))
   Stilde <- -(log((nn0 - nd0)/nn0) + jpar[1]) / jpar[2]

   CalibS0.fit <- optim(par=c(jpar, Stilde), 
                       fn=CalibS0.LLik,  nd=Target.dat$detect,
                       S=Target.dat$SQ, nn=Target.dat$n, nd0=nd0, nn0=nn0,
                       control=list(fnscale=-1), method="BFGS",
                       gr=CalibS0.dLLik, hessian=TRUE)
   

  CalibS0.fit.Var <- solve(-CalibS0.fit$hessian)
  betaS <- CalibS0.fit$par
  hess <- CalibS0.ddLLik(betaS, Target.dat$detect, Target.dat$SQ,
                Target.dat$n, nd0, nn0)
  
  cmat <- cbind(betaS, sqrt(diag(CalibS0.fit.Var)))
  cmat <- cbind(cmat, cmat[,1]/cmat[,2])
  cmat <- cbind(cmat, 2*pnorm(-cmat[, 3]))
  if (nrow(cmat)==2) {rownames(cmat) <- c("beta", "SQ0")
  } else  rownames(cmat) <- c("alpha","beta", "SQ0")
  colnames(cmat) <- c("Estimate", "Std.Err", "Z value", "Pr(>z)")

  cat('\n\n','ML estimate of SQ for nd0 number of detects and nn0 replicates', '\n')
  if(sink.indicator) sink(file(paste("Outputs\\MLSQ",uTargets[i],".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  cat(as.character(uTargets[i]), 'y0=', nd0, 'n0=' , nn0, "\n\n")
  printCoefmat(cmat, digits=3)
  if(sink.indicator) sink()

  #ML Double check calculations
  # Shat <- -(log((nn0 - nd0)/nn0) + betaS[1]) / betaS[2]
  # cat('\n', 'Shat=', Shat)
  # cat('\n', 'SEs ', sqrt(diag(solve(-hess))))
  # cat('\n', 'Marginal ', (1/sqrt(-hess[3,3])))
  # cat('\n', 'Gradient ', CalibS0.dLLik(betaS, Target.dat$detect, Target.dat$SQ, 
  #                   Target.dat$n, nd0, nn0), '\n')

  CalibS0.fit.estimates[[i]] <- cmat
  
}

```

## Estimate predicted SQ given consecutive numbers of detects given number of technical replicates - intercept model
The estimated SQ is easily obtained for given new values of nn0=number of replicates, 
nd0=number detected and the estimated slope betaS 
as:    Shat <- -(log((nn0 - nd0)/nn0) + betaS[1]) / betaS[2]
The standard errors are obtained from the Hessian matrix 
(or via the function CalibS0.ddLLik()).

```{r MLES0fitsvec, warning=FALSE, echo=FALSE}  

  #Calibration estimate of S0 given consecutive numbers of detects given vector
  # number of technical replicates nn0vec - intercept model
  # includes standard errors

i <- 1 #ML Only 1 Target used here, eONKI4
nn0vec <- c(8, 16, 24, 32)  #ML cycle through vector number of technical replicates
CalibS0.table <- vector("list", length(nn0vec))  #list of fits including test value
  for(j in 1:length(nn0vec)) {
     nn0 <- nn0vec[j]
     if(nrow(nndetect[[i]]) < 3) {CalibS0.table[[i]] <- NULL
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}
   
  Target.dat <- nndetect[[i]]
  SQS0.fit <- matrix(0, ncol=6, nrow=0)
  
  for(nd0 in 0:(nn0-1)){

   jlm <- lm(lamhat ~ SQ, data=Target.dat) #starting values for alpha and beta
   jpar <- pmax(c(0.01, 0.01), coef(jlm))
   Stilde <- -(log((nn0 - nd0)/nn0) + jpar[1]) / jpar[2]

   CalibS0.fit <- optim(par=c(jpar, Stilde), 
                       fn=CalibS0.LLik,  nd=Target.dat$detect,
                       S=Target.dat$SQ, nn=Target.dat$n, nd0=nd0, nn0=nn0,
                       control=list(fnscale=-1), method="BFGS",
                       gr=CalibS0.dLLik, hessian=TRUE)

  CalibS0.fit.Var <- solve(-CalibS0.fit$hessian)
  betaS <- CalibS0.fit$par
  cvec <- as.vector(rbind(betaS, sqrt(diag(CalibS0.fit.Var))))
  SQS0.fit <- rbind(SQS0.fit, cvec)
  }
  
  rownames(SQS0.fit) <- paste(0:(nn0-1))
  colnames(SQS0.fit) <- c("alpha", "SEalpha", "beta","SEbeta", "SQ0","SE_SQ0")
  
  #Negative SQ0 values can occur when there are detects for negative controls
  # set SQ0 and SE_SQ0 to zero
  SQS0.fit[ SQS0.fit[,5] < 0, 5:6] <- 0
  
  cat("\n",as.character(uTargets[i]), "\n")
  cat(' ML estimate of SQ for numbers of detects and', nn0, 'replicates', '\n')
  if(sink.indicator) {
    sink(file(paste("Outputs\\MLSQ0",uTargets[i],".txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  }
  print(knitr::kable(cbind(NumDetects=0:(nn0-1), SQS0.fit[,5:6]), 
                     format="pandoc", digits=3),results="asis")

  if(sink.indicator) sink()
  
  CalibS0.table[[j]] <- SQS0.fit  
  }

```



\newpage

## Lc, Ld, Lq's - no intercept model

### Determine Lc, Ld, Lq (LOB, LOD, LOQ) - no intercept model
Follows Lavagnini and Magno 2007, Mass Spectrometry Reviews
*The notation in our eDNA paper is different from the Lavagnini 2007 paper and is shown in brackets here.*

- Lc (*LOB Limit of blank*) = critical level is the assay signal above which a response is reliably attributed to the presence of analyte 
- Ld (*Ld = expected number detects out of NN replicates at concentration LOD*)  = signal corresponding to an analyte concentration xd (*=LOD Limit of Detection*) level which may be a priori expected to be recognized
- Lq = quantification limit is a signal with a precision which satisfies an expected value ($=\gamma_Q$)

Lc corresponds to a critical response level or a false positive rate,
i.e. critical number of detects given NN replicates, 
above which we would reject the null hypothesis that the concentration/copy number is zero at
alpha = alphaLc ($=\gamma_{FP}$).  It is the critical response level corresponding to the false positive rate
of alphaLc.  Essentially, the test is positive if the Y~Binomial(m, p) > Lc.
The False Positive Rate is P(Y > Lc | S=0).

Ld is computed to correspond to the false negative rate, beta = betaLd ($=\gamma_{FN}$) here.  It is computed so
that the probability of observing a new (unknown concentration) response less than or equal to 
Lc is less than or equal to betaLd.  The probability of observing Lc or less detects if the
concentration is xd (*=LOD Limit of Detection*) or more is less than or equal to betaLd.  The values of Lc depend on
the number of replicates, NN, so xd does as well.  Ld is the expected number of detects at
values xd (*=LOD Limit of Detection*) and NN replicates. False negative rate Ld computation:  P(Y <= Lc | p_xd) <= betaLd, ($=\gamma_{FN}$)
and solve for xd (*=LOD Limit of Detection*).
 

Lq is less well defined.  The literature suggests using Lq = beta0 + 10 s.e.(beta0), but this
uses the normality assumption.  Other literature suggests using the "analyte concentration xq (*=LOQ Limit of Quantification*)
for which the experimental relative standard deviation of the responses reaches a fixed level ($=\gamma_Q$),
for example, the level 0.1." Lavagnini and Magno 2007.  I interpret the term "relative
standard deviation" to mean the coefficient of variation, CV = sd/mean.

In the exercise below, we use the fits from the ML models to estimate the Lc, Ld and Lq, for
various values of NN replicates for a new observation, i.e. a new (unknown concentration) response
number of detects.
Both the intercept and no intercept models are considered.

```{r LcLdLqNN0, echo=FALSE}
# No intercept model computations - Here Lc==0
# Lc computation:  P(Y > Lc | S=0) <= alphaLc 
#  where Y ~ Bin(m, p=1 - exp(-betas[1]))
#  to incorporate estimation uncertaintly in betas[1], use upper limit of conf int
#  i.e. Y ~ Bin(m, p=1 - exp(-(betas[1] + 1.96 * s.e.(betas[1])))
# Ld computation:  P(Y <= Lc | p_xd) <= betaLd
#  use relationship between Binomial and Beta distribution, p 278 Bain
# Lq computation:  (Forootan 2017)  choose level, Sj, such that CV<=gammaLq
#  not clear which scale: response, back-transformed values, ... ?
#
##* Note in R:  The quantile is defined as the smallest value x such 
##    that F(x) ≥ p, where F is the distribution function.

## Set values for alphaLC, betaLd, gammaLq
alphaLc <- betaLd <- .05; gammaLq <- .20
NN <- c(3, 8, 16, 24, 32, 48, 64, 96)  #test number of replicates

#set up tables for manuscript output
Lc.all0 <- matrix(0, nrow=nTargets, ncol=length(NN))
row.names(Lc.all0) <- uTargets
colnames(Lc.all0) <- paste(NN)
xdd.all0 <- xd.all0 <- xd_upper.all0 <- xd_lower.all0 <- xq.all0 <- xq_upper.all0 <- 
  xq_lower.all0 <- Lc.all0 


for(i in 1:nTargets){
  #use beta estimated from fits above
     if(nrow(nndetect[[i]]) < 3) {
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}

  betas <- (Calib.fit.estimates0[[i]])[,1]
  
  #ML Takes into account uncertainty in new observation and s.e. of betas
  betas.upper <- betas + 1.96 * (Calib.fit.estimates0[[i]])[,2]
  betas.lower <- betas - 1.96 * (Calib.fit.estimates0[[i]])[,2]
  
  #Want: P(Y > Lc | S=0) <= alphaLc where Y ~ Bin(m, p=1 - exp(-betas[1]))
  #Lc at xc=0 values for new observation
  
  # For no intercept model, P(Y = 0 | S=0)=1 and P(i'th tech rep detect| S=0)=0
  #  Lc==0, and the P(Y > Lc | S=0) <= alphaLc
  #  since P(Y > 0 | S=0)==0.  
  # We are saying that sample is negative if Y=0 and positive if Y>0

  Lc <- rep(0, length(NN))

  #Want xd, P(Y <= Lc | p_xd) <= betaLd
  #Ld and xd calculation
  pxd <- 1 - qbeta(betaLd, NN-Lc, Lc+1)   #proportion detected
  Ld <- NN * pxd
  xd <- ( - log(1 - pxd)) / betas   #concentration
  xdlower <- ( - log(1 - pxd)) / betas.upper
  xdupper <- ( - log(1 - pxd)) / betas.lower
  names(pxd) <- names(xd) <- names(xdlower) <- names(xdupper) <- paste(NN)
  
  #Compute confidence interval for xd
  xd.all0[i,] <- xd
  xd_upper.all0[i,] <- xdupper
  xd_lower.all0[i,] <- xdlower

  #Compute model based xq
  xq <- -(log(1 - 1/(1 + gammaLq^2*NN)))/betas
  names(xq) <- paste(NN)

  #Compute model based xq using lower estimates of beta - upper bound for xq
  xq_lower <- -(log(1 - 1/(1 + gammaLq^2*NN)))/betas.lower
  names(xq_lower) <- paste(NN)
 
  #Compute model based xq using upper estimates of beta
  xq_upper <- -(log(1 - 1/(1 + gammaLq^2*NN)))/betas.upper
  names(xq_upper) <- paste(NN)

  xq.all0[i,] <- xq
  xq_lower.all0[i,] <- xq_lower
  xq_upper.all0[i,] <- xq_upper

  }


  
#For a given p_hat, one can compute the sample size m required to attain a
#  CV  <= gammaLq, as   m >= (1-phat)/(phat*gammaLq)

```

### Print out LOD (no intercept model) with confidence intervals, by number of replicates

```{r Ld0all, echo=FALSE}
j <- rbind(xd.all0[i,], xd_lower.all0[i,], xd_upper.all0[i,])
rownames(j) <- c('LOD', 'LOD_lower', 'LOD_upper')
print(round(j, 2), digits=2)
```


### Determine Lc, Ld, Lq (LOB, LOD and LOQ) - intercept model


```{r LcLdLqNN, echo=FALSE}
# Lc computation:  P(Y > Lc | S=0) <= alphaLc - intercept model computations
#  where Y ~ Bin(m, p=1 - exp(-betas[1]))
#  to incorporate estimation uncertaintly in betas[1], use upper limit of conf int
#  i.e. Y ~ Bin(m, p=1 - exp(-(betas[1] + 1.96 * s.e.(betas[1])))
# Ld computation:  P(Y <= Lc | p_xd) <= betaLd
#  use relationship between Binomial and Beta distribution, p 278 Bain
# Lq computation:  (Forootan 2017)  choose level, Sj, such that CV<=gammaLq
#  not clear which scale: response, back-transformed values, ... ?
#
##* Note in R:  The quantile is defined as the smallest value x such 
##    that F(x) ≥ p, where F is the distribution function.

## Set values for alphaLC, betaLd, gammaLq
alphaLc <- betaLd <- .05; gammaLq <- .20
NN <- c(3, 8, 16, 24, 32, 48, 64, 96)  #test number of replicates

#set up tables for manuscript output
Lc.all <- matrix(0, nrow=nTargets, ncol=length(NN))
row.names(Lc.all) <-  uTargets
colnames(Lc.all) <-  paste(NN)
Lc.upper.all <- xd.all <- xd_upper.all <- xd_lower.all <- 
  xq.all <- xq_lower.all <- xq_upper.all <- Lc.all 

for(i in 1:nTargets){
  #use beta estimated from fits above
     if(nrow(nndetect[[i]]) < 3) {
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}

  betas <- (Calib.fit.estimates[[i]])[,1]
  
  #ML Takes into account uncertainty in new observation and s.e. of betas
  betas.upper <- betas + 1.96 * (Calib.fit.estimates[[i]])[,2]
  betas.lower <- pmax(0, betas - 1.96 * (Calib.fit.estimates[[i]])[,2])
  
  #Want: P(Y > Lc | S=0) <= alphaLc where Y ~ Bin(m, p=1 - exp(-betas[1]))
  #Lc at xc=0 values for new observation
  p.new <- 1 - exp(-betas[1])
  Lc.new <- qbinom(1 - alphaLc, size=NN, prob=p.new)
  names(Lc.new) <- paste(NN) 
  Lc.all[i,] <- Lc.new
  
  #Lc.upper at xc=0 values for new observation, incl s.e. of betas
  p.upper <- 1 - exp(-betas.upper[1])
  Lc.upper <- qbinom(1 - alphaLc, size=NN, prob=p.upper)
  names(Lc.upper) <- paste(NN) 
  Lc.upper.all[i,] <- Lc.upper
  
  #Want xd, P(Y <= Lc | p_xd) <= betaLd
  #Ld and xd calculation
  pxd <- 1 - qbeta(betaLd, NN-Lc.new, Lc.new+1)   #proportion detected
  Ld <- NN * pxd
  xd <- (-betas[1] - log(1 - pxd)) / betas[2]   #concentration
  names(pxd) <- names(xd) <- paste(NN)

  # pxd_upper <- 1 - qbeta(betaLd, NN-Lc.upper, Lc.upper+1)   #proportion detected\
  # Ld_upper <- NN * pxd_upper
  # xd_upper <- (-betas[1] - log(1 - pxd_upper)) / betas[2]   #concentration
  xd_lower <- pmax(0, (-betas.upper[1] - log(1 - pxd)) / betas.upper[2])
  xd_upper <- (-betas.lower[1] - log(1 - pxd)) / betas.lower[2]
  names(xd_upper) <- names(xd_lower) <- paste(NN)

  xd.all[i,] <- xd
  xd_upper.all[i,] <- xd_upper
  xd_lower.all[i,] <- xd_lower
  
  #Compute model based xq
  xq <- -(betas[1] + log(1 - 1/(1 + gammaLq^2*NN)))/betas[2]
  names(xq) <- paste(NN)

  #Compute model based xq using lower estimates of beta
  xq_lower <- -(betas.lower[1] + log(1 - 1/(1 + gammaLq^2*NN)))/betas.lower[2]
  names(xq_lower) <- paste(NN)
  
  #Compute model based xq using upper estimates of beta
  xq_upper <- -(betas.upper[1] + log(1 - 1/(1 + gammaLq^2*NN)))/betas.upper[2]
  names(xq_upper) <- paste(NN)


  xq.all[i,] <- xq
  xq_lower.all[i,] <- xq_lower
  xq_upper.all[i,] <- xq_upper


  }


  
#For a given p_hat, one can compute the sample size m required to attain a
#  CV  <= gammaLq, as   m >= (1-phat)/(phat*gammaLq)

```

### Print out LOD (intercept model) with confidence intervals, by number of replicates

```{r Ldall, echo=FALSE}

jj <- rbind(Lc.all[i,], xd.all[i,], xd_lower.all[i,], xd_upper.all[i,])
rownames(jj) <- c('LOB', 'LOD', 'LOD_lower', 'LOD_upper')
print(round(jj, 2), digits=2)
```

\pagebreak

### Estimates, Lc, Ld, Lq (LOB, LOD, LOQ) and confidence limits for a given number of technical reps NN[NNi]

Chooses the model (intercept versus no intercept) with the best LLR test fit,
i.e. the largest p-value for the LLR test.  A table of values for all
assays is printed.  

```{r ChooseModel, echo=FALSE}
# Set the index into NN which is defined in chunk LcLdLqNN0 and LcLdLqNN  
# NNi <- 2 corresponds to the 2nd entry of NN
NNi <- 2

#cat('Limits intercept model for N=', NN[NNi])
xdxq.all <- cbind(LOB=Lc.all[,NNi], 
                  # LcUp=Lc.upper.all[,NNi],
                   LOD_Low=xd_lower.all[,NNi],  
                   LOD=xd.all[,NNi], LOD_Up=xd_upper.all[,NNi],
                   LOQ_Low=xq_upper.all[,NNi],
                   LOQ=xq.all[,NNi], LOQ_Up=xq_lower.all[,NNi])


#cat('Limits for no intercept model for N=', NN[NNi])
xdxq.all0 <- cbind(LOB=0, LOD_Low=xd_lower.all0[,NNi],  
                   LOD=xd.all0[,NNi], LOD_Up=xd_upper.all0[,NNi],
                   LOQ_Low=xq_upper.all0[,NNi], 
                   LOQ=xq.all0[,NNi], LOQ_Up=xq_lower.all0[,NNi])

#Include alpha and beta estimates in table
alphabeta.se <- alphabeta0.se  <- matrix(0, nrow=nTargets, ncol=4)
colnames(alphabeta.se) <- c("alpha","aSE", "beta", "bSE")
colnames(alphabeta0.se) <- c("alpha","aSE", "beta", "bSE")
rownames(alphabeta0.se) <- rownames(alphabeta.se) <- uTargets
for (i in 1:nTargets){
  if(nrowTarget[i]>2){
  alphabeta.se[i,1:2] <- Calib.fit.estimates[[i]][1, 1:2]
  alphabeta.se[i, 3:4] <- Calib.fit.estimates[[i]][2, 1:2]
  alphabeta0.se[i, 3:4] <- Calib.fit.estimates0[[i]][1, 1:2]
  }
}



Calib.choice <- Calib.fit.LLRp > Calib.fit.LLRp0
xdxq.choice <- cbind(InterModel=Calib.choice, alphabeta0.se, xdxq.all0)
xdxq.choice[Calib.choice, 6:12] <- xdxq.all[Calib.choice,]
xdxq.choice[Calib.choice, 2:5] <- alphabeta.se[Calib.choice,]

# NNi <- 2 This is set above
cat('Limits for best choice model for N=', NN[NNi], '\n')
print(round(xdxq.choice[nrowTarget > 2,], digits=2))



```


\pagebreak

## eDNA Manuscript Tables and Graphs (not used in *fishes* manuscript)

*Revised for general use to use all eligible targets. *  
*Revised to output results for both intercept and no intercept models.*

```{r Manuscript, echo=FALSE, eval=TRUE, include=FALSE}
ManuTargets <- (1:nTargets)[nrowTarget > 2] #choose all eligible targets
Manusink <- FALSE  #writes to output files when set to TRUE
#Manusink <- TRUE

# Best model table
{
if(Manusink){ 
      sink(file(paste("Outputs\\Limits.txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
}
xdxq.choice.PR <- xdxq.choice[nrowTarget>2, -1, drop=FALSE]
xdxq.choice.PR <- xdxq.choice.PR[order(row.names(xdxq.choice.PR)), , drop=FALSE]
colnames(xdxq.choice.PR)[5:11] <- c('LOB','LODL','LOD','LODU','LOQL','LOQ','LOQU')
print(knitr::kable(round(xdxq.choice.PR, 
                    digits=1), 
                    format="pandoc", digits=1, 
#   caption='Estimates, Critical, Detection, Quantification Limits and CIs',
                     results="asis"))
if(Manusink){ sink()}
}

# Intercept table
{
if(Manusink){ 
      sink(file(paste("Outputs\\LimitsAll.txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
}
xdxq.all.PR <- cbind(alphabeta.se, xdxq.all)
xdxq.all.PR <- xdxq.all.PR[nrowTarget>2, , drop=FALSE]
xdxq.all.PR <- xdxq.all.PR[order(row.names(xdxq.all.PR)), , drop=FALSE]
colnames(xdxq.all.PR)[5:11] <- c('LOB','LODL','LOD','LODU','LOQL','LOQ','LOQU')
print(knitr::kable(round(xdxq.all.PR, 
                    digits=1), 
                    format="pandoc", digits=1, 
   caption='Intercept model: Estimates, Critical, Detection, Quantification Limits and CIs',
                     results="asis"))
if(Manusink){ sink()}
}

# No Intercept table
{
if(Manusink){ 
      sink(file(paste("Outputs\\LimitsAll0.txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
}
xdxq.all0.PR <- cbind(alphabeta0.se, xdxq.all0)
xdxq.all0.PR <- xdxq.all0.PR[nrowTarget>2, , drop=FALSE]
xdxq.all0.PR <- xdxq.all0.PR[order(row.names(xdxq.all0.PR)), , drop=FALSE]
colnames(xdxq.all0.PR)[5:11] <- c('LOB','LODL','LOD','LODU','LOQL','LOQ','LOQU')
print(knitr::kable(round(xdxq.all0.PR, 
                    digits=1), 
                    format="pandoc", digits=1, 
   caption='No intercept model: Estimates, Critical, Detection, Quantification Limits and CIs',
                     results="asis"))
if(Manusink){ sink()}
}


# Data to file
{
if(Manusink){
  sink(file("Outputs\\Data.txt", encoding="UTF-8"),
       split=TRUE)
  }
for(Targeti in ManuTargets){
  jmat <- nndetect[[Targeti]][, c(2:4, 12, 10)]
  row.names(jmat) <- NULL
  names(jmat) <- c("S", "num.detect", "n", "p.tilde", "lambda.tilde")
  print(knitr::kable(jmat, 
                     format="pandoc", digits=3, caption=uTargets[Targeti]),
                     results="asis")
}
if(Manusink){ sink()}
}

# Print all regression outputs
{
 if(Manusink){
    sink(file(paste("Outputs\\Results.txt", sep=""), 
            encoding="UTF-8"), split=TRUE)
  }
# Print regression outputs
for(Targeti in ManuTargets){
#  if (Calib.choice[Targeti]){
  cat("\n", as.character(uTargets[Targeti]), "\n")
  cat("Convergence=", Calib.fit.res$convergence[Targeti], "\n")
  printCoefmat(Calib.fit.estimates[[Targeti]], digits=3)
  cat('LLR test stat=', Calib.fit.res$LLR[Targeti], ', df= ', 
      Calib.fit.res$degf[Targeti], ', p-value=', Calib.fit.res$Pval[Targeti], "\n\n")
#  } else {
   cat("\n", as.character(uTargets[Targeti]), "\n")
   cat("Convergence=", Calib.fit.res0$convergence[Targeti], "\n")
   printCoefmat(Calib.fit.estimates0[[Targeti]], digits=3)
   cat('LLR test stat=', Calib.fit.res0$LLR[Targeti], ', df= ', 
      Calib.fit.res0$degf[Targeti], ', p-value=', Calib.fit.res0$Pval[Targeti], "\n\n")
#  }
}
if(Manusink) sink()
}

## Manuscript plots to file
# pdf('Outputs\\gBlockPlots.pdf')
par(mfrow=c(1,2))
#par(mfrow=c(2,2))

# Plots of fits using chosen no-intercept/intercept model
#  Revision - plot all output results
for(Targeti in ManuTargets){
  Target.dat <- nndetect[[Targeti]]
  
#  if (Calib.choice[Targeti]){
  Calib.fit <- Calib.fit.all[[Targeti]]
  Calib.fitted <- Calib.fit$par[1] + Calib.fit$par[2]*  Target.dat$SQ  
  
  # Plot calibration curve on lambda scale
  maxSQ <- max(Target.dat$SQ)
  maxlamhat <- max(Target.dat$lamhat)

  plot(Target.dat$SQ, Target.dat$lamhat, xlog=TRUE, 
       ylab='Mean Copy Estimate',
       xlab='Starting copy number',
       ylim=c(0, 4), xlim=c(0, maxSQ), las=1, 
#      ylim=c(0, maxlamhat), xlim=c(0, maxSQ), las=1, 
       main=uTargets[Targeti])
  abline(Calib.fit$par[1], Calib.fit$par[2], col=4)
  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  arrows(Target.dat$SQ, Target.dat$Lamhatex.Lower, Target.dat$SQ, 
         Target.dat$Lamhatex.Upper,
         length=0.05, angle=90, code=3)

  # Plot calibration curve on phat scale
  # Compute minSQ such that phat~=1 (1-phat = .99)
  maxSQa <-max( -(Calib.fit$par[1] + log(.01))/Calib.fit$par[2], maxSQ)
  sqs <- seq(0, maxSQa, by=.1)
  Calib.phat <- 1 - exp(-(Calib.fit$par[1] + Calib.fit$par[2] * sqs))

  plot(sqs, Calib.phat, xlog=TRUE, 
       ylab='Proportion detect', xlab='Starting copy number', type='l', col=4, las=1, 
       ylim=c(0, 1), xlim=c(0, maxSQa),  main=paste('Intercept', uTargets[Targeti]))
  points(Target.dat$SQ, Target.dat$phat)
  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  
 # } else {

  #Compute fitted values for ML model
  Calib.fit <- Calib.fit.all0[[Targeti]]
  Calib.fitted <- Calib.fit$par *  Target.dat$SQ
  
  # Plot calibration curve on lambda scale
  maxSQ <- max(Target.dat$SQ)
  maxlamhat <- max(Target.dat$lamhat)

  plot(Target.dat$SQ, Target.dat$lamhat, xlog=TRUE, 
       ylab='Mean Copy Estimate', xlab='Starting copy number',
       ylim=c(0, 4), xlim=c(0, maxSQ), las=1, 
 #     ylim=c(0, maxlamhat), xlim=c(0, maxSQ), las=1, 
      main=uTargets[Targeti])
  abline(0, Calib.fit$par, col=4)
  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  arrows(Target.dat$SQ, Target.dat$Lamhatex.Lower, Target.dat$SQ, 
         Target.dat$Lamhatex.Upper,
         length=0.05, angle=90, code=3)

  # Plot calibration curve on phat scale
  # Compute minSQ such that phat~=1 (1-phat = .99)
  maxSQa <-max( -( log(.01))/Calib.fit$par, maxSQ)
  sqs <- seq(0, maxSQa, by=.1)
  Calib.phat <- 1 - exp(-( Calib.fit$par * sqs))

  plot(sqs, Calib.phat, xlog=TRUE, 
       ylab='Proportion detect', xlab='Starting copy number', type='l', col=4, las=1, 
       ylim=c(0, 1), xlim=c(0, maxSQa),  main=paste('No intercept', uTargets[Targeti]))
  points(Target.dat$SQ, Target.dat$phat)
  legend("topleft", legend=c('ML fit'), lty=1, col=4, bty="n")
  
}
  
  cat("\n\n")
#}

# Plot limits of detection
# Note:  the same vertical scale is used for all assays
# Revised to plot all output results
max.xd_upper <- max(xd_upper.all[ManuTargets,], xd_upper.all0[ManuTargets,])
for(Targeti in ManuTargets){
#max.xq_lower <- max(xq_lower.all[Targeti,])

#  if (Calib.choice[Targeti]){
  plot(NN, xd.all[Targeti,], ylab='LOD',
       xlab='m, number of replicates', xaxt='n', 
       ylim=c(0, max.xd_upper), xlim=c(min(NN), max(NN)),  las=1,   
       main=paste('Limits detect - intercept', uTargets[Targeti]))
#     main=uTargets[Targeti])
  axis(1, at=NN)
 
  # Transformed Exact CI
  arrows(NN, xd_lower.all[Targeti,], NN, 
        xd_upper.all[Targeti,],
         length=0.05, angle=90, code=3)
 # } else {

#max.xq_lower <- max(xq_lower.all0[Targeti,])

  plot(NN, xd.all0[Targeti,], ylab='LOD',
       xlab='m, number of replicates', xaxt='n', 
       ylim=c(0, max.xd_upper), xlim=c(min(NN), max(NN)),  las=1,   
       main=paste('Limits detect - no intercept', uTargets[Targeti]))
#      main=uTargets[Targeti])
  axis(1, at=NN)
 
  # Transformed Exact CI
  arrows(NN, xd_lower.all0[Targeti,], NN, 
        xd_upper.all0[Targeti,],
         length=0.05, angle=90, code=3)
}
  
  cat("\n\n")
  
  
#}

# Plots limits of Quantification
# Note:  the same vertical scale is used for all assays
# Revised to plot all results
max.xq_lower <- max(xq_lower.all[ManuTargets,], xq_lower.all0[ManuTargets,])
for(Targeti in ManuTargets){

#  if (Calib.choice[Targeti]){
  plot(NN, xq.all[Targeti,], ylab='LOQ',
       xlab='m, number of replicates', xaxt='n', 
       ylim=c(0, max.xq_lower), xlim=c(min(NN), max(NN)),  las=1,   
       main=paste('Limits quant - intercept', uTargets[Targeti]))
#      main=uTargets[Targeti])
  axis(1, at=NN)
 
  # Transformed Exact CI
  arrows(NN, xq_upper.all[Targeti,], NN, 
        xq_lower.all[Targeti,],
         length=0.05, angle=90, code=3)
 # } else {

#max.xq_lower <- max(xq_lower.all0[Targeti,])

  plot(NN, xq.all0[Targeti,], ylab='LOQ',
       xlab='m, number of replicates', xaxt='n', 
       ylim=c(0, max.xq_lower), xlim=c(min(NN), max(NN)),  las=1,   
       main=paste('Limits quant - no intercept', uTargets[Targeti]))
#      main=uTargets[Targeti])
  axis(1, at=NN)
 
  # Transformed Exact CI
  arrows(NN, xq_upper.all0[Targeti,], NN, 
        xq_lower.all0[Targeti,],
         length=0.05, angle=90, code=3)
}
  
  cat("\n\n")
#}

#dev.off()
par(mfrow=c(1,1))
```







# Cq gBlock section, calibration and estimation


## Medians for CqT=Transformed Cq and Cq


```{r CTdat, echo=FALSE}
# Define transformed Cq, setting nondetects to 0
DATz.df <- subset(DAT.df, DAT.df$SQ>4, drop=FALSE)
print(paste('number of zero Cqs: ', sum(is.na(DATz.df$Cq))))

DATz.df <- within(DATz.df, {
      CqT <- 50 - Cq
      L10.SQ <- log10(SQ)
})
DATz.df$CqT[is.na(DATz.df$CqT)] <- 0

CqT.med <- tapply(DATz.df$CqT, list(DATz.df$Target, DATz.df$SQ), median)
kable(CqT.med, format="pandoc", digits=2, 
      caption="Transformed Cq medians by SQ (nondetect = 0)")

Cq.med <- tapply(DATz.df$Cq, list(DATz.df$Target, DATz.df$SQ), median, na.rm=TRUE)
kable(Cq.med, format="pandoc", digits=2, 
      caption="Cq medians by SQ (nondetect removed)")

```

## Plot transformed Cq versus log_10 SQ

```{r TranCq, echo=FALSE, fig.height=7, fig.width=8}
# Plot Transformed Cq versus log_10 SQ to determine relationship
### Red triangles are medians.
### code was written to process several assays

 if(nTargets>3) par(mfrow=c(2,2))

 for (i in 1:nTargets){
   ind <- DATz.df$Target==uTargets[i]
   with(DATz.df[ind,], plot(jitter(L10.SQ), CqT, ylim=c(0,max(CqT)),
           ylab="CqT, median", xlab=expression(log[10](SQ)), main=uTargets[i]))
   indmed <- rownames(CqT.med)==uTargets[i]
    L10.USQ <- log10(as.numeric(colnames(CqT.med)))
   points(L10.USQ, CqT.med[indmed,], pch=2, col=2, cex=1.5)
   cat("\n\n")
 }

```

## High copy standard curve using Weighted Least Squares and gBlock data
Fit regression models to Cq's for calibration of SQ on log_10 SQ scale
using method of Lavignini et al. 2007, bottom page 11.  The R code names
follow the
notation of Laivgnini et al. 2007.  For example, Xwhat is $\hat{X_w}$. 


Aside:
Kralik, Petr and Ricchi, Matteo (2017) "A basic guide to real time PCR
in microbial diagnostics:  Definitions, parameters, and everything",
Frontiers in Microbiology, 1-9.

Mathematical principle of qPCR.  

$N_n = N_0 \times (1 + E)^n$.

$N_n$ is the number of PCR amplicons after $n$ cycles,
$N_0$ is the initial number of template copies in the sample,
E is the PCR efficiency that can assume values in the range from 0 to 1,
n is the number of cycles.




```{r calib, echo=FALSE}
# This code is specific for eONKI4 gBlock data
if(nTargets>2) par(mfrow=c(1,2))

# coefficient of variation function
cvar <- function(x){sd(x, na.rm=TRUE)/abs(mean(x, na.rm=TRUE))}

#Calib.rlist <- vector("list", length=nTargets)  #Robust results
Calib.list <- Calib.wlist <- vector("list", length=nTargets)  #results
L10.SQ.CVs <- vector("list", length=nTargets)  #CVs of L10 SQ estimates
DATz.df$Cq.var <- 0  #initialize within SQ sample variances

for(i in 1:nTargets) {
#  indcal <- DATz.df$Target==UTargets[i] & DATz.df$SQ >=LOD.mod[i] #uses data >= LOD.mod
  indcal <- DATz.df$Target==uTargets[i] & DATz.df$SQ > 4 #uses data SQ > 4, eONKI4 data

  #Robust model not used
  # jrmod <- robust::lmRob(Cq ~ L10.SQ, data=DATz.df, subset=indcal) #Robust regression
  # with(DATz.df[indcal,], plot(L10.SQ, Cq, ylim=c(0, max(c(50, Cq), na.rm=TRUE)),  
  #         ylab="Cq", xlab="SQ", main=UTargets[i], xaxt="n"))
  # axis(1, at=seq(-1 ,5), labels=paste(round(10^seq(-1,5), 1)) )
  # abline(coef(jrmod), col=2)
  # jrab <- coef(jrmod)

# Weighted robust regression - weights are inversely proportional to variances
#   Compute sample variances

  cq.var <- with(DATz.df[indcal,], tapply(Cq, list(SQ), var, na.rm=TRUE)) 
  ul10SQ <- log10(as.numeric(names(cq.var)))
  DATz.df$Cq.var[indcal] <- with(DATz.df[indcal,], 
       cq.var[match(SQ, as.numeric(names(cq.var)))])

# Model for the variances/weights for inverse regression 
# Can be used to estimate weights for a new sample
  plot(ul10SQ, log(cq.var), xlab='log_10 SQ', main='Cq variances versus log_10 SQ')
  cq.var.lm <-lm(log(cq.var) ~ ul10SQ + I(ul10SQ^2) )  #var model for Cq given log_10 SQ
  summary(cq.var.lm)
  lines(ul10SQ, fitted(cq.var.lm))  
cat('Model for the variances/weights for inverse regression which can be used to estimate weights for a new sample.', "\n")


# OLS and WLS regression for calibration model
#  the values in weights being inversely proportional to the variances
  Cj <- lm(Cq ~ L10.SQ, data=DATz.df, subset=indcal)  #OLS
  Cjw <- lm(Cq ~ L10.SQ, weights=1/Cq.var, data=DATz.df, subset=indcal) #WLS
  print(summary(Cjw))
  Cjw.ab <- coef(Cjw)
  Cjs <- summary(Cj)
  Cjws <- summary(Cjw)

#Quantities from the weighted calibration curve
summary(Cjws$residuals)  #unweighted residuals
w <- Cjws$weights
X <- Cjw$model$L10.SQ
Y <- Cjw$model$Cq

sxyw2 <- sum(w*Cjws$residuals^2)/Cjw$df.residual #weighted sigma^2, line (21)
sxyw <- sqrt(sxyw2)
Xwbar <- sum(w*X)/sum(w)  #weighted mean
SSwX <- sum(w * (X - Xwbar)^2)

#CIs for inverse regression
cq.mean <- with(DATz.df[indcal,], tapply(Cq, list(SQ), mean, na.rm=TRUE)) #y mean
cq.m <- with(DATz.df[indcal,], tapply(Cq, list(SQ), length))  #length of y, m
Xwhat <-(cq.mean - Cjw.ab[1]) / Cjw.ab[2]  #X weighted hat on log_10 SQ scale
ywhatlim <- Cjw.ab[1] + Xwhat * Cjw.ab[2]  # this is just cq.mean

#weights are 1/cq.var from just before equation (23)
#se's for ywhat and Xwhat
ywhat.se <- sxyw * sqrt((cq.var/cq.m) + 1/sum(w) +
        (Xwhat - Xwbar)^2/SSwX)
Xwhat.se <- ywhat.se / abs(Cjw.ab[2])   #approximation

#95% CI for Ywhat, then Xwhat - first method bottom of page 11 just before equation (23)
ywhatlimL <- ywhatlim +   qt(.975, Cjw$df.residual) * ywhat.se
ywhatlimU <- ywhatlim -   qt(.975, Cjw$df.residual) * ywhat.se

XwhatlimU <- (ywhatlimU - Cjw.ab[1]) / Cjw.ab[2]  #confidence limits for Xwhat
XwhatlimL <- (ywhatlimL - Cjw.ab[1]) / Cjw.ab[2]
cbind(ul10SQ, XwhatlimL, Xwhat, XwhatlimU) #true SQ, Lower Limit, Estimated, Upper Limit



# Calibration plot Ct
  with(DATz.df[indcal,], plot(L10.SQ, Cq, ylim=c(20, max(c(Cq), na.rm=TRUE)),
          ylab="Ct", xlab="Starting copy number", 
          #ML main=uTargets[i], 
          main='Ct versus SQ, weighted regression',
          xaxt="n", las=1))
  axis(1, at=seq(-1 ,5), labels=paste(round(10^seq(-1,5), 1)) )
  abline(Cjw.ab, col=2)
  
   cat("\n\n")
  
#Hatchery plot, calibration plot Transformed Ct
  with(DATz.df[indcal,], plot(L10.SQ, CqT, 
          ylab="Transformed Ct", xlab="Starting copy number per reaction", 
          #ML main=uTargets[i],
          main='Transformed Ct versus SQ, weighted regression',
          xaxt="n", las=1))
  axis(1, at=seq(-1 ,5), labels=paste(round(10^seq(-1,5), 1)) )
  abline(c(50, 0) - Cjw.ab, col=1)
  
   cat("\n\n")

#Confirm CqT weighted model with Cjws weighted model for Cq 
  jwlmT <- lm(CqT ~ L10.SQ, weights=1/Cq.var, data=DATz.df, subset=indcal)
 print(summary(jwlmT))

# #Hatchery paper, transposed calibration plot Transformed Ct - not used
#   with(DATz.df[indcal,], plot( CqT, L10.SQ,
#           xlab="Transformed Ct", ylab="Starting copy number per reaction",
#           #ML main=uTargets[i],
#           main='SQ versus Transformed Ct, weighted regression',
#           yaxt="n", las=1) )
#   axis(2, at=seq(-1 ,5), labels=paste(round(10^seq(-1,5), 1)) )
#   abline(c(50 - Cjw.ab[1], -1)/Cjw.ab[2], col=1)
# # Confidence intervals for SQ0 based on the number of tech replicates in the gblock
# #  lines(50 - cq.mean, XwhatlimU,  col=2, lty=2)  
# #  lines(50 - cq.mean, XwhatlimL,  col=2, lty=2)



# Aside
# Efficiency, E, Kralik, Petr and Ricchi, Matteo (2017)
  cat(as.character(uTargets[i]),' E=', 10^(-1/Cjw.ab[2]) - 1, 'efficiency', '\n' )
  cat(as.character(uTargets[i]), 'OLS coefs ', coef(Cj), '\n')
  cat(as.character(uTargets[i]), 'WLS coefs ', coef(Cjw), '\n')

    
# CV, mean and sd on the log10(SQ) scale
  L10.SQ.CV <- with(DATz.df[indcal,], 
                    tapply((Cq - Cjw.ab[1])/Cjw.ab[2], list(SQ), cvar))
  L10.SQ.mean <- with(DATz.df[indcal,], 
                    tapply((Cq - Cjw.ab[1])/Cjw.ab[2], list(SQ), mean, na.rm=TRUE))
  L10.SQ.sd <- with(DATz.df[indcal,], 
                    tapply((Cq - Cjw.ab[1])/Cjw.ab[2], list(SQ), sd, na.rm=TRUE))

  cat(as.character(uTargets[i]),'CVs by SQ',"\n")
  print(L10.SQ.CV)

#  Calib.rlist[i] <- list(jrmod)  #Robust list of results
  Calib.list[i] <- list(Cj)     #lm list of results
  Calib.wlist[i] <- list(Cjw)     #lm weighted list of results
  L10.SQ.CVs[i] <- list(L10.SQ.CV)
  cat("\n\n")
}
#names(Calib.rlist) <- UTargets 
names(Calib.list) <- uTargets
names(Calib.wlist) <- uTargets
names(L10.SQ.CVs) <- uTargets

# save(Calib.list, file='Calib.list' )
# save(Calib.rlist, file='Calib.rlist' )
# save(Calib.wlist, file='Calib.wlist' )

par(mfrow=c(1,1))
```

# Density Experiment
## Load in data from file 'eco'
## Estimate Copy number for Density data using weighted LS in chunk 'calib'

```{r DenInverse, echo=FALSE}
# Ran Calib-Density-ML-date.Rmd first to create density data; data saved as
# save(list=c('eco.dat', 'eco.SC', 'eco.SC.TF', 'eco.SC.F'), file='eco')
# Summarised data over:  SC (sort code) sample; TF (Tank, Fish); F (Fish)
# Note:  missing TCTs were set to zero; missing CTs were set to NA

# Estimate copy number using weighted LS model from chunk 'calib' above
# uses method of Lavagnini et al. 2007, bottom page 11

#Density data saved here. eco.SC.TF is average of sort code means by (Fish, Tank)
load('eco')    
summary(eco.SC.TF)

Cjw <- Calib.wlist$eONKI4  #weighted lm object computed in chunk 'calib'
Cjw.ab <- coef(Cjw)
Cjws <- summary(Cjw)

#Quantities from the weighted calibration curve
# summary(Cjws$residuals)  #unweighted residuals
w <- Cjws$weights
X <- Cjw$model$L10.SQ
Y <- Cjw$model$Cq

sxyw2 <- sum(w*Cjws$residuals^2)/Cjw$df.residual #weighted sigma^2, line (21)
sxyw <- sqrt(sxyw2)
Xwbar <- sum(w*X)/sum(w)  #weighted mean
SSwX <- sum(w * (X - Xwbar)^2)

#New data for inverse regression; estimated Xwhat on log_10 copy number scale
Xwhat <- with(eco.SC.TF, (CTSCTFmean - Cjw.ab[1]) / Cjw.ab[2]) #X w(eighted) hat on log_10 scale
ywhatlim <- Cjw.ab[1] + Xwhat * Cjw.ab[2] #Y w(eighted) hat = CTSCTFmean


# Could use **variance model cq.var.lm** to get weight in ywhat.se at estimated Xwhat
# to get se's for ywhat and Xwhat.
# Estimated variance at Xwhat from calib chunk, there were 8 reps for each
#var.Xwhat <- exp(predict(cq.var.lm, data.frame(ul10SQ=Xwhat))) 
#ywhat.se <- with(eco.SC.TF, sxyw * sqrt((var.Xwhat/(numSCTF*8)) + 1/sum(w) +

#CTSCTFsd is the sd over the numSCTF CTSCmean's, i.e. over 5 or 4 samples
ywhat.se <- with(eco.SC.TF, sxyw * sqrt((CTSCTFsd^2/numSCTF) + 1/sum(w) +
        (Xwhat - Xwbar)^2/SSwX))
Xwhat.se <- ywhat.se / abs(Cjw.ab[2])

#95% CI for Xwhat - first method bottom of page 11 before equation (23)
ywhatlimL <- ywhatlim +   qt(.975, Cjw$df.residual) * ywhat.se
ywhatlimU <- ywhatlim -   qt(.975, Cjw$df.residual) * ywhat.se

XwhatlimU <- (ywhatlimU - Cjw.ab[1]) / Cjw.ab[2] #confidence limits for Xwhat
XwhatlimL <- (ywhatlimL - Cjw.ab[1]) / Cjw.ab[2]
# cbind(XwhatlimL, Xwhat, XwhatlimU, eco.SC.TF$CTSCTFsd) #true SQ, Lower Limit, Estimated, Upper Limit

# # Log_10 plot not used
# with(eco.SC.TF, plot(l2Bio, Xwhat, ylab=expression(Log[10]~copy~number),
#      xlab=expression(Coho~biomass~(log[2]~g)),
#      ylim=c(0, 3), col=as.numeric(TankF), pch=as.numeric(TankF), 
#      las=1, main='Density, Estimated Log_10 Copy versus Log_2 Biomass'))


Xl2what <- Xwhat * log2(10) #convert log10 to log2 scale
with(eco.SC.TF, plot(l2Bio, Xl2what, ylab=expression(Log[2]~copy~number),
     ylim=c(0, 10), col=as.numeric(TankF), pch=as.numeric(TankF), las=1,
     xlab=expression(Coho~biomass~(log[2]~g)),
#    main='Density, Estimated Log_2 Copy versus Log_2 Biomass'
    ))
with(eco.SC.TF, legend('topleft', c('Tank 19', 'Tank 20', 'Tank 21', 'Tank 24'), 
       col=as.numeric(TankF), pch=as.numeric(TankF), bty='n'))


# Weighted regression of (Xwhat) log_2 estimated copy number by log_2 biomass
#  note se's for Xwhat computed on log_10 scale, however,
#  weighted regression results should not change after multiplication 
#  of weights by a constant, log2(10)
Copy.wlm <- lm(Xl2what ~ l2Bio, weights=(1/Xwhat.se^2), data=eco.SC.TF)
abline(Copy.wlm)
summary(Copy.wlm)

l2fish <- with(eco.SC.TF, tapply(l2Bio, FishF, mean ) )
xfish <- c(1,2,4,8,16,32,65)
axis(side=3, at=l2fish, labels=paste(xfish))
mtext('Number fish per 10,000L tank', side=3, line=-1, outer=TRUE)

# set up new data for confidence interval for fitted line
l2Bio.new <- data.frame(l2Bio=with(eco.SC.TF, 
                  seq(min(l2Bio), max(l2Bio), length=28)))

l2Bio.new <- cbind(l2Bio.new, predict(Copy.wlm, l2Bio.new, interval="confidence" ))

# Mean log_2 copy numbers estimated from calibration curve given Cq by Fish
j2 <- tapply(Xl2what, eco.SC.TF$FishF, mean)
j2 <- rbind(j2, 2^j2)
rownames(j2) <- c('mean log2 Copy','mean copy')

# Estimated copy numbers and log_2 copy numbers from Density curve given Number Fish
j3 <- predict(Copy.wlm, data.frame(l2Bio=l2fish))
Density.p.copy <- rbind(j2, predict_copy=2^j3, predict_log2=j3)
cat("Model estimates(predict) and actuals by Number of Fish", "\n")
Density.p.copy 

```

### Ggplot density plot

```{r, GGDensity, warning=FALSE, message=FALSE, echo=FALSE, fig.height=7, fig.width=6.5}
colors <- c( "Reg line"="blue")

gDensity <- ggplot(data=eco.SC.TF) +
  theme_bw() +
  geom_point(mapping=aes(x=l2Bio, y=Xl2what), shape=0) +
  # geom_abline(slope=1, intercept=0) +
  # geom_segment(aes(x=min(l2Bio), y=min(l2Bio), xend=max(l2Bio), yend=max(l2Bio),
  #                  color="Line of equality"), size=.9) +
  theme(legend.position = "none") +
  labs(color="") +
  scale_color_manual(values = colors) +
  geom_smooth(aes(x=l2Bio.new$l2Bio, y=l2Bio.new$fit, ymin=l2Bio.new$lwr, 
                  ymax=l2Bio.new$upr, color="Reg line"), stat="identity") +
  xlab("Number of Fish per 10 kL Tank") +
  ylim(0, 10) +
  xlim(0, 10) +
  ylab(expression(Log[2]~copy~number/L)) +
  theme(panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black")) +
  # scale_x_continuous(labels=c(2.5, 5, 7.5), breaks=c(2.5, 5, 7.5), 
  #                    position = "bottom") +
  scale_x_continuous(sec.axis=sec_axis(trans=~ . * 1,
        name=TeX("Coho biomass ($log_{2} 10^{-4}$ g/L)")),
        labels=xfish, breaks=l2fish, position='top') +
   theme(axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15),
        axis.title=element_text(size=15))

gDensity
# ggsave('Outputs\\Density.png', dpi=300)
# ggsave('Outputs\\Density.jpg', dpi=300)
# ggsave('Outputs\\Density.tiff', dpi=300)

```

# Dilution/Flow experiment 

## Load data from file flowFT; estimate Copy number for Flow/Tank Binomial data

Uses CalibS0.table0 generated using no intercept model for eONKI4 in chunk
'MLES0fits0vec' above.  Estimate SQ given number of detects in data flow.FT.


```{r SQ0Binomialflow, echo=FALSE}
#flowFT data created in Calib-RFlowData-12Nov2019-RS-MLcut.Rmd.

# Dilution (Flow) series averaged by (Flow, Tank)
load('flowFT')
cat("Dilution data by Flow and Tank", "\n")
flow.FT

#choose flows/tanks with nondetects
flow.FTBinom <- droplevels(flow.FT[flow.FT$FlowN >=40,])  
#dim(flow.FTBinom)
#summary(flow.FTBinom)
#flow.FTBinom  # FlowN, numFT, ndetects

#Esimate SQ0 using  CalibS0.table0  no intercept gBlock model
#Need table of SQ0 and SE_SQ0 for numFT = 8, 16, 24, 32, nn0vec0 <- c(8, 16, 24, 32)
#CalibS0.table0

flow.FTBinom <- cbind(flow.FTBinom, SQ0=0, SE_SQ0=0)
for(k in 1:nrow(flow.FTBinom)){
#ML   flow.FTBinom[k, 11:12] <-  (CalibS0.table[nn0vec==flow.FTBinom[k, 'numFT']])[[1]][1+flow.FTBinom[k,'ndetects'], 5:6]
#ML switch to no intercept model
        
  flow.FTBinom[k, 11:12] <-  (CalibS0.table0[nn0vec0==flow.FTBinom[k, 'numFT']])[[1]] [1+flow.FTBinom[k,'ndetects'], 3:4]

}
cat("Data with Flow>=40 and Binomial-Poisson copy number estimates, SQ0", "\n")
flow.FTBinom #Binomial copy number estimates

```


## Estimate Copy number for high copy number Flow/Dilution data

Uses calibration weighted linear regression model from above, chunk 'calib'   

```{r FlowWeighted, echo=FALSE}
# uses method of Lavignini et al. 2007, bottom page 11
#ML May30,2022 - Omit Tank 21, Flow 20, Sort.code=21, DPN=15A - go back to 
#  Calib-RFlowData-12Nov2019.RS-MLcut.Rmd

#load('flowFT') loaded above
flow.FTCq <- droplevels(flow.FT[flow.FT$FlowN < 40,])
#dim(flow.FTCq)
#flow.FTCq
#summary(flow.FTBinom) #Binomial estimates

#load('Calib.wlist')  #gBlock CT fits using weighted regression

Cjw <- Calib.wlist$eONKI4  #weighted lm object from above chunk 'calib'
Cjw.ab <- coef(Cjw)
Cjws <- summary(Cjw)

#Quantities from the weighted calibration curve
#summary(Cjws$residuals)  #unweighted residuals
w <- Cjws$weights
X <- Cjw$model$L10.SQ   #X is on the log_10 scale
Y <- Cjw$model$Cq

sxyw2 <- sum(w * Cjws$residuals^2)/Cjw$df.residual #weighted sigma^2 eq(21)
sxyw <- sqrt(sxyw2)
Xwbar <- mean(w*X)/sum(w)  #weighted mean
SSwX <- sum(w * (X - Xwbar)^2) #denominator in (20)

#New data for inverse regression
#summary(flow.FTCq)
Xwhat <- with(flow.FTCq, (CTFTmean - Cjw.ab[1]) / Cjw.ab[2]) #p11, section B
ywhatlim <- Cjw.ab[1] + Xwhat * Cjw.ab[2] 

#new weights are 1/CTSCTFsd^2
#se's for ywhat and Xwhat
ywhat.se <- with(flow.FTCq, sxyw * sqrt((CTFTsd^2/numFT) + 1/sum(w) +
        (Xwhat - Xwbar)^2/SSwX))
Xwhat.se <- ywhat.se / abs(Cjw.ab[2])  

#95% CI for Xwhat
ywhatlimL <- ywhatlim +   qt(.975, Cjw$df.residual) * ywhat.se
ywhatlimU <- ywhatlim -   qt(.975, Cjw$df.residual) * ywhat.se

XwhatlimU <- (ywhatlimU - Cjw.ab[1]) / Cjw.ab[2]
XwhatlimL <- (ywhatlimL - Cjw.ab[1]) / Cjw.ab[2]
# cbind(XwhatlimL, Xwhat, XwhatlimU, eco.SC.TF$CTSCTFsd)

# SQ estimates for low flows
flow.FTCq <- cbind(flow.FTCq, SQest=10^Xwhat, L10SQest=Xwhat, L10SQse=Xwhat.se)
cat("High copy number Dilution data with copy number estimated, SQest", "\n")
flow.FTCq

#Plot on log_2 copy number by log_2 flow scale
Xl2what <- Xwhat * log2(10) #on log2 scale
XXwhat <- 10^Xwhat  #on copy number scale
#as.matrix(XXwhat)
with(flow.FTCq, plot(log2(flow.FTCq$FlowN), Xl2what, ylab='log 2 Copy number',
     xlab='log 2 (Flow)', xlim=c(min(log2(flow.FT$FlowN)), max(log2(flow.FT$FlowN)) ),
     ylim=c(-10, 10), col=as.numeric(TankF), pch=as.numeric(TankF), las=1))

with(flow.FTBinom, points(log2(flow.FTBinom$FlowN), log2(SQ0), 
            col=as.numeric(TankF), pch=as.numeric(TankF)))

title('Log2 Copy Number versus log2 Flow')

 cat("\n\n")

#Plot on copy number by log_2 flow scale
with(flow.FTCq, plot(log2(flow.FTCq$FlowN), XXwhat, ylab='Copy number',
     xlab=expression(Log[2]~(Flow)), 
     xlim=c(min(log2(flow.FT$FlowN)), max(log2(flow.FT$FlowN)) ),
     ylim=c(0, max(XXwhat)),
     #ML ylim=c(-10, 10), 
     col=as.numeric(TankF), pch=as.numeric(TankF), las=1))

with(flow.FTBinom, points(log2(flow.FTBinom$FlowN), SQ0, 
            col=as.numeric(TankF), pch=as.numeric(TankF)))

with(flow.FTBinom, legend('topright', c('Tank 19', 'Tank 20', 'Tank 21', 'Tank 24'), 
       col=as.numeric(TankF), pch=as.numeric(TankF), bty='n'))

title('Copy Number versus log2 Flow')

 cat("\n\n")

```

## Extra 40Kl Dilution Table

```{r flow40, echo=FALSE}
#Flow data from Calib-RFlowData-12Nov2019-RS-MLcut.Rmd

# flow.save <- flow.cut
# flow.save$FlowN <- as.numeric(flow.save$Flow)
# flow.save$FlowF <- as.factor(flow.save$Flow)
# flow.save$TankF <- as.factor(flow.save$Tank)
# flow.save$TCT <- flow.save$Transformed.Ct
# flow.save$CT <-  50.001 - flow.save$TCT
# flow.save$CT[flow.save$CT==50] <- NA
# save('flow.save', file='flow')

load('flow')
# Separate out the 40Kl flow samples for detects by sort code
# Estimate copy number using both models, Cq model for 100% detects
# Binomial-Poisson model for <100% detects - get these from CalibS0.table0 table above

flow.40 <- flow.save %>%  
  filter(Flow==40 ) %>%
  group_by(Sort.Code) %>%
  summarise(Tank=Tank[1], TankF=TankF[1], FlowN=FlowN[1], FlowF=FlowF[1],
     TCTSCmean=mean(TCT, na.rm=TRUE), TCTSCsd=sd(TCT, na.rm=TRUE),
     CTSCmean=mean(CT, na.rm=TRUE), CTSCsd=sd(CT, na.rm=TRUE), numSC=n(),
     ndetects=sum(!is.na(CT)))
flow.40 <- droplevels(data.frame(flow.40))
summary(flow.40)

# From Cq gBlock model
Xwhat.40 <- 10^with(flow.40, (CTSCmean - Cjw.ab[1]) / Cjw.ab[2]) #p11, section 
#Xwhat.40

cat("Use Weighted regression Cq model to estimate copy number Xwhat.40 for 100% detects","\n")
cat("Ignore Xwhat.40 for <100% detects and use Binomial-Poisson model instead." ,"\n")
cbind(ndetects=flow.40$ndetects, CTmean=flow.40$CTSCmean, TCTmean=flow.40$TCTSCmean, Xwhat.40)

```

# Background estimation 

## Background estimate from Density experiment negative controls


```{r backzeroesdensity, echo=FALSE}
# Extract zero fish CT scores from density experiment
eco.zerodat <- eco.dat %>% filter(Fish==0)
#summary(eco.zerodat)
eco.zeroCTmean <- mean(eco.zerodat$CT, na.rm=TRUE)
eco.zeroCTsd <- sd(eco.zerodat$CT, na.rm=TRUE)
eco.zeroCTnum <- sum(!is.na(eco.zerodat$CT))

#ML Use Poisson-Binomial model to estimate SQ given eco.zeroCTnum detects
#  out of all 424
#Calib.fit.all0[[1]]$par 
SQDen <- - log(1 - eco.zeroCTnum/nrow(eco.zerodat))/Calib.fit.all0[[1]]$par
cat('Density background estimate:  ',SQDen,"\n")


# Estimate SQ given individual samples
# summary by Sort.Code
eco.zerodat.SC <- eco.zerodat %>% 
   group_by(Sort.Code) %>%
   summarise(Tank=Tank[1], TankF=TankF[1], Fish=Fish[1], FishF=FishF[1], 
     nd=sum(!is.na(CT)), numSC=n()
   )
eco.zerodat.SC <- droplevels(data.frame(eco.zerodat.SC))
#eco.zerodat.SC
cat('Density negative controls, ndetect, nreps',"\n")
apply(eco.zerodat.SC[,c('nd','numSC')], 2, sum)

eco.back <- with(eco.zerodat.SC,
                 - log(1 - nd/numSC)/Calib.fit.all0[[1]]$par
)

# Mean estimated background for Density experiment using individual samples
SQB <- mean(eco.back)  
SQB.sd <- sd(eco.back)
cat('Mean of individual Density background estimates:  ', SQB, "\n")
#SQB.sd

```

## Background estimate from Dilution data

Estimate of background using flow negative controls.

```{r backzeroesflow, echo=FALSE}


#summarise Zeroes by Sort.Code
flow.SCZ <- flow.save %>%  
  filter(Flow==0 & Flow !='sink' & Flow!='pond' ) %>%
  group_by(Sort.Code) %>%
  summarise(Tank=Tank[1], TankF=TankF[1], FlowN=FlowN[1], FlowF=FlowF[1],
     TCTSCmean=mean(TCT, na.rm=TRUE), TCTSCsd=sd(TCT, na.rm=TRUE),
     CTSCmean=mean(CT, na.rm=TRUE), CTSCsd=sd(CT, na.rm=TRUE), numSC=n(),
     ndetects=sum(!is.na(CT)))
flow.SCZ <- droplevels(data.frame(flow.SCZ))
flow.SCZ

#ML Use Poisson-Binomial model to estimate SQ given detects

# Mean estimated background for Flow experiment using individual samples
flow.back <- with(flow.SCZ,
                 - log(1 - ndetects/numSC)/Calib.fit.all0[[1]]$par)
SQBflow <- mean(flow.back)  #mean flow estimated background
SQBflow.sd <- sd(flow.back)
cat('Mean background of individual flow negative controls:  ', SQBflow, "\n")
#SQBflow.sd
```

## Background estimate from Density and Dilution data

```{r backzeroesBoth, echo=FALSE}

#ML Use Poisson-Binomial model to estimate SQ given detects
#  density + flow detects (22+6)=28 out of (424+96)=520 technical reps

zdetect <- eco.zeroCTnum + sum(flow.SCZ$ndetects)
znum <- nrow(eco.zerodat) + sum(flow.SCZ$numSC)

SQBboth <- - log(1 - zdetect/znum)/Calib.fit.all0[[1]]$par

cat('Background copy number estimate:  ',SQBboth, "\n")
cat('ndetects:  ', zdetect, 'nreps: ',znum, "\n")

```



# BACKGROUND Limits of Detection using estimated background SQBboth, No intercept model

For the ordinary Limit of detection, Lc (or LOB) is the critical value for
the test of the null hypothesis that SQ=0.  Here the null hypothesis is
that SQB=S_Background (using SQBboth) and we compute a critical value LOB-B, Limit of Background
Blank. We also compute LOD-B, Limit of Background Detection.

```{r BackLcLdLqNN0, echo=FALSE}
# No intercept model computations - Here Lc==0
# Lc computation:  P(Y > Lc | S=0) <= alphaLc 
#  where Y ~ Bin(m, p=1 - exp(-betas[1]))
#  to incorporate estimation uncertaintly in betas[1], use upper limit of conf int
#  i.e. Y ~ Bin(m, p=1 - exp(-(betas[1] + 1.96 * s.e.(betas[1])))
# Ld computation:  P(Y <= Lc | p_xd) <= betaLd
#  use relationship between Binomial and Beta distribution, p 278 Bain
# Lq computation:  (Forootan 2017)  choose level, Sj, such that CV<=gammaLq
#  not clear which scale: response, back-transformed values, ... ?
#
##* Note in R:  The quantile is defined as the smallest value x such 
##    that F(x) ≥ p, where F is the distribution function.

## Set values for alphaLC, betaLd, gammaLq
alphaLc <- betaLd <- .05; gammaLq <- .20
NN <- c(3, 8, 16, 24, 32, 40, 48, 64, 96)  #test number of replicates

#set up tables for manuscript output
#  Background copy number estimated above SQBoth
LcB.all0 <- matrix(0, nrow=nTargets, ncol=length(NN))
row.names(LcB.all0) <- uTargets
colnames(LcB.all0) <- paste(NN)
xddB.all0 <- xdB.all0 <- xdB_upper.all0 <- xdB_lower.all0 <- xqB.all0 <- xqB_upper.all0 <- 
  xqB_lower.all0 <- LcB.upper.all0 <- LcB.all0 


for(i in 1:nTargets){
  #use beta estimated from fits above
     if(nrow(nndetect[[i]]) < 3) {
     cat(paste('Too few values for ', uTargets[i]), '\n') 
     next}

  betas <- (Calib.fit.estimates0[[i]])[,1]
  
  #ML Takes into account uncertainty in new observation and s.e. of betas
  betas.upper <- betas + 1.96 * (Calib.fit.estimates0[[i]])[,2]
  betas.lower <- betas - 1.96 * (Calib.fit.estimates0[[i]])[,2]
  
# SQBboth <- - log(1 - 28/520)/Calib.fit.all0[[1]]$par #computed above
  cat('Background estimate and confidence interval \n \n')
  SQB.upper <- - log(1 - 28/520)/betas.lower
  SQB.lower <- - log(1 - 28/520)/betas.upper
  #sQB computed above
  cat(SQBboth, "  (", SQB.lower, SQB.upper, ")", "\n") 

  
  #Want: P(Y > LcB | S=SQBboth) <= alphaLc where Y ~ Bin(m, p=1 - exp(-betas[1]*SQB))
  #?? LcB at xc=SQB values for new observation
  ##ML update, now use SQBboth
  
  # We are saying that sample is Background negative if Y<=LcB and positive if Y>LcB

  p.new <- 1 - exp(-betas[1]*SQBboth)
  LcB.new <- qbinom(1 - alphaLc, size=NN, prob=p.new)
  names(LcB.new) <- paste(NN) 
  LcB.all0[i,] <- LcB.new
  
  #Lc.upper at xcB=SQBboth values for new observation, incl s.e. of betas
  pB.upper <- 1 - exp(-betas.upper[1]*SQBboth)
  LcB.upper <- qbinom(1 - alphaLc, size=NN, prob=p.upper)
  names(LcB.upper) <- paste(NN) 
  LcB.upper.all0[i,] <- LcB.upper
  


  #Want xdB, P(Y <= LcB | p_xdB) <= betaLd
  #LdB and xdB calculation
  pxdB <- 1 - qbeta(betaLd, NN-LcB.new, LcB.new+1)   #proportion detected
  LdB <- NN * pxdB
  xdB <- ( - log(1 - pxdB)) / betas   #concentration
  xdBlower <- ( - log(1 - pxdB)) / betas.upper
  xdBupper <- ( - log(1 - pxdB)) / betas.lower
  names(pxdB) <- names(xdB) <- names(xdBlower) <- names(xdBupper) <- paste(NN)
  
  #Compute confidence interval for xdB
  xdB.all0[i,] <- xdB
  xdB_upper.all0[i,] <- xdBupper
  xdB_lower.all0[i,] <- xdBlower
  
  }


  

```

## Print out BACKGROUND limits of detection with confidence intervals, by number of replicates

```{r BackLBd0, echo=FALSE}
LcBLdB <- rbind(LcB.all0[i,], ( - log(1 - LcB.all0[i,]/NN)) / betas, 
                xdB.all0[1,], xdB_lower.all0[i,], xdB_upper.all0[i,])
rownames(LcBLdB) <- c('LOB-B', 'LOB-Bcn', 'LOD-B', 'LOD-B_lower', 'LOD-B_upper')
print(round(LcBLdB, 2), digits=2)

LcBLdB <- data.frame(LcBLdB)

```

## Fit Bent Cable model to flow data mean over (Flow/Tank) copy numbers

```{r BentCableModel, echo=FALSE}
#Bent cable  model on log 2 Copy number scale using mean copy over Tanks by Flow
y.cp <- c(XXwhat, flow.FTBinom$SQ0)
y.cp.mean <- tapply(y.cp, flow.FT$FlowN, mean)
y.cp.sd <- tapply(y.cp, flow.FT$FlowN, sd)

flow.logbent <- bent.cable(unique(log2(flow.FT$FlowN)), log2(y.cp.mean))
x.grid <- seq(min(log2(flow.FT$FlowN)), max(log2(flow.FT$FlowN)), 
              length=200)
flow.xlogfit <- predict(flow.logbent, x.grid)

# Generate confidence bands for bentcable model
#  Uses the inherent linear model with x and q 
#  See code for bent.cable

alphahat <- flow.logbent$alpha 
cat("Breakpoint alphahat:  ",alphahat, ", 2^alphahat:  ", 2^alphahat,"\n")  #breakpoint estimate
gammahat <- flow.logbent$gamma

  I.middle <- 1 * (abs(x.grid - alphahat) < gammahat)
  I.left <- 1 * (x.grid >= alphahat + gammahat)
  qhat <- (x.grid - alphahat + gammahat)^2/(4 * gammahat) * I.middle + 
    (x.grid - alphahat) * I.left

xq.grid <- data.frame(x.grid, qhat)
names(xq.grid)=c('x','q')

flow.xlogfitCI <- data.frame(predict(flow.logbent$model, newdata=xq.grid, 
                          interval='confidence'))

summary(flow.logbent$model)
```

## Intersections with Bent cable model

```{r BentIntersections, echo=FALSE}
# Computed above
# flow.xlogfitCI <- data.frame(predict(flow.logbent$model, newdata=xq.grid, 
#                           interval='confidence'))

## X-value from Bentcable model for LdB when num tech replicates is 40 and 8
L2LOBD40 <- log2(LcBLdB$X40[3])
L2LOBD8 <- log2(LcBLdB$X8[3])
L2LOBD24 <- log2(LcBLdB$X24[3])
L2Lcx40 <- log2(LcBLdB$X40[2])
L2Lcx8 <- log2(LcBLdB$X8[2])
L2Lcx24 <- log2(LcBLdB$X24[2])

# Fitted values from chunk RobbylogFlow above
bentfit <- cbind(xq.grid, flow.xlogfitCI)

#LOD-B:  locate x for L2LOBD40, 8
#L2LOBD40
flow40 <- bentfit[sort(abs(bentfit$fit - L2LOBD40), index.return=T)$ix[1], 1 ]

#L2LOBD8
flow8 <- bentfit[sort(abs(bentfit$fit - L2LOBD8), index.return=T)$ix[1], 1]

#L2LOBD24
flow24 <- bentfit[sort(abs(bentfit$fit - L2LOBD24), index.return=T)$ix[1], 1]


#Background estimate:  locate x for log2(SQBboth) for the first part of the curve
#log2(SQBboth)
flowB <- bentfit[sort(abs(bentfit$fit[bentfit$x < 7.5] - log2(SQBboth)), 
                      index.return=T)$ix[1], 1]

#Lc on x scale:  locate x for log2(LcBx) for the first part of the curve
flowLcx40 <- bentfit[sort(abs(bentfit$fit[bentfit$x < 7.5] - L2Lcx40), 
                      index.return=T)$ix[1], 1]

flowLcx8 <- bentfit[sort(abs(bentfit$fit[bentfit$x < 7.5] - L2Lcx8), 
                      index.return=T)$ix[1], 1]

flowLcx24 <- bentfit[sort(abs(bentfit$fit[bentfit$x < 7.5] - L2Lcx24), 
                      index.return=T)$ix[1], 1]


j <- cbind(Log2copy=c(log2(SQBboth), L2LOBD8, L2Lcx8, L2LOBD24,  L2Lcx24, L2LOBD40, L2Lcx40),
           Log2Vol=c(flowB[1], flow8[1], flowLcx8,  flow24[1], flowLcx24, flow40[1],  flowLcx40))
rownames(j) <- c('Background', 'LODB_8','Lcx_8', 'LODB_24','Lcx_24','LODB_40','Lcx_40')

j <- cbind(j, Copy=2^j[,1], VolKL=2^j[,2])
'Bent cable model intersections'
j
```


## GGplot log2 Flow/Dilution plot using mean copy numbers

```{r BentCablePlot, warning=FALSE, echo=FALSE, fig.height=7, fig.width=8}
# Set linetypes for ggplot legend
linetypes <- c( "LOD-B" = "dotted", 
               # "LOD-B upper" = "dotted",
               # "LOD-B lower" = "dotted",
               "LOB-Bcn" = 'dotdash',
               "Background" = "longdash")

glogFlow24 <- ggplot() +
   theme_bw() +
   geom_point(aes(x=unique(log2(flow.FT$FlowN)), y=log2(y.cp.mean)),shape=0) +
   ylim(-8, 10) +
   geom_line(aes(x=x.grid, y=flow.xlogfit), size=1, col=2) +
   geom_errorbar(aes(x=unique(log2(flow.FT$FlowN)),
                     ymin=log2(y.cp.mean - y.cp.sd), 
                     ymax=log2(y.cp.mean + y.cp.sd)),
                 width=.1) +
   # geom_segment(aes(x=3, y=.07, xend=5.3, yend=.07)) +
   geom_ribbon(aes(x=x.grid, ymin=flow.xlogfitCI$lwr, 
                   ymax=flow.xlogfitCI$upr), alpha=0.1)+
   geom_line(aes(x=x.grid, y=log2(SQBboth),     #Computed SQBboth above
                linetype='Background'), color="blue", size=1) +
   geom_line(aes(x=x.grid, y=L2Lcx24,     #Computed above
                linetype='LOB-Bcn'), color="blue", size=1) +

  
   # geom_line(aes(x=x.grid, y=log2(SQB.lower),     #Computed above
   #              linetype='Background'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(SQB.upper),     #Computed above
   #              linetype='Background'), color="blue", size=1) +
 
  
   # geom_line(aes(x=x.grid, y=log2(SQB + SQB.sd),   #Compute SQB.sd above
   #               linetype='U Background'), color="blue", size=1) +
   
#LOD-B for 8 technical reps is 1.05 with upper limit of 1.62, computed above
#LOD-B for 40 technical reps is .32 (CI .24, .50)
#LOD-B for 24 technical reps is .40 (CI .29, .61)

   geom_line(aes(x=x.grid, y=log2(.40),   #Computed above
                  linetype='LOD-B'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(.61),   #Computed above
   #                linetype='LOD-B upper'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(0.29),   #Computed above
   #                linetype='LOD-B lower'), color="blue", size=1) +
   scale_linetype_manual(values = linetypes) +
   xlab("Log2 Water Volume (KL) diluted") +
   ylab(expression(Log[2]~copy~number/L)) +
   labs(linetype="") +
   theme(panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black")) + 
   scale_x_continuous(name=TeX("$Log_2$ Water Volume (kL) diluted"), 
                      breaks=3:10, 
                      labels=paste(3:10)) + 
   theme(legend.position = "top") +
   theme(axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15),
        axis.title=element_text(size=15),
        legend.text=element_text(size=13))
#  scale_y_continuous(expand = c(0,0))

glogFlow24
cat("24 technical replicates", "\n")

# ggsave('Outputs\\Dilution24.png', glogFlow24,  units="in", height=6, width=9, dpi = 300)
# ggsave('Outputs\\Dilution24.jpg', glogFlow24,  units="in", height=6, width=9, dpi = 300)
# ggsave('Outputs\\Dilution24.tiff', glogFlow24,  units="in", height=6, width=9, dpi = 300)


glogFlow8 <- ggplot() +
  theme_bw() +
   geom_point(aes(x=unique(log2(flow.FT$FlowN)), y=log2(y.cp.mean)),shape=0) +
   ylim(-8, 10) +
   geom_line(aes(x=x.grid, y=flow.xlogfit), size=1, col=2) +
   geom_errorbar(aes(x=unique(log2(flow.FT$FlowN)),
                     ymin=log2(y.cp.mean - y.cp.sd), 
                     ymax=log2(y.cp.mean + y.cp.sd)),
                 width=.1) +
   # geom_segment(aes(x=3, y=.07, xend=5.3, yend=.07)) +
   geom_ribbon(aes(x=x.grid, ymin=flow.xlogfitCI$lwr, 
                   ymax=flow.xlogfitCI$upr), alpha=0.1)+
   geom_line(aes(x=x.grid, y=log2(SQBboth),     #Computed SQBboth above
                linetype='Background'), color="blue", size=1) +
   geom_line(aes(x=x.grid, y=L2Lcx8,     #Computed above
                linetype='LOB-Bcn'), color="blue", size=1) +

   # geom_line(aes(x=x.grid, y=log2(SQB.lower),     #Computed above
   #              linetype='Background'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(SQB.upper),     #Computed above
   #              linetype='Background'), color="blue", size=1) +
 
  
   # geom_line(aes(x=x.grid, y=log2(SQB + SQB.sd),   #Compute SQB.sd above
   #               linetype='U Background'), color="blue", size=1) +
   
#LOD-B for 8 technical reps is 1.05 with CI (.78, 1.62), computed above

   geom_line(aes(x=x.grid, y=log2(1.05),   #Computed above
                  linetype='LOD-B'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(1.62),   #Computed above
   #                linetype='LOD-B upper'), color="blue", size=1) +
   # geom_line(aes(x=x.grid, y=log2(0.78),   #Computed above
   #                linetype='LOD-B lower'), color="blue", size=1) +
   scale_linetype_manual(values = linetypes) +
   xlab("Log2 Water Volume (KL) diluted") +
   ylab(expression(Log[2]~copy~number/L)) +
   labs(linetype="") +
   theme(panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black")) + 
   scale_x_continuous(name=TeX("$Log_2$ Water Volume (kL) diluted"), 
                      breaks=3:10, 
                      labels=paste(3:10)) + 
   theme(legend.position = "top") +
   theme(axis.text.x = element_text(size = 15),
        axis.text.y = element_text(size = 15),
        axis.title=element_text(size=15),
        legend.text=element_text(size=13))

#  scale_y_continuous(expand = c(0,0))

glogFlow8
cat("8 technical replicates", "\n")

# ggsave('Outputs\\Dilution8.png', glogFlow8, units="in", height=6, width=9, dpi = 300)
# ggsave('Outputs\\Dilution8.jpg', glogFlow8, units="in", height=6, width=9, dpi = 300)
# ggsave('Outputs\\Dilution8.tiff', glogFlow8, units="in", height=6, width=9, dpi = 300)

```



